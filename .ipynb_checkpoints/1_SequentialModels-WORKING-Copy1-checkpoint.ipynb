{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import Linear, Embedding, RNN, GRU, LSTM\n",
    "from torch.nn import Sigmoid, LogSoftmax\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss, NLLLoss, CrossEntropyLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>set</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\tolivetti does not exclude stake in sgs thoms...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>\\tolivetti does not exclude stake in sgs thoms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\tu k firm ups italy fund ita stake to pct llo...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>\\tu k firm ups italy fund ita stake to pct llo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\tmoniterm corp mtrm th qtr loss shr loss cts ...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>\\tmoniterm corp mtrm th qtr loss shr loss cts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\twal mart stores inc wmt raises quarterly qtl...</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "      <td>\\twal mart stores inc wmt raises quarterly qtl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\tautospa lube to buy control of cardis cds au...</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>\\tautospa lube to buy control of cardis cds au...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label    set  \\\n",
       "0  \\tolivetti does not exclude stake in sgs thoms...      1  train   \n",
       "1  \\tu k firm ups italy fund ita stake to pct llo...      1  train   \n",
       "2  \\tmoniterm corp mtrm th qtr loss shr loss cts ...      0  train   \n",
       "3  \\twal mart stores inc wmt raises quarterly qtl...      0  train   \n",
       "4  \\tautospa lube to buy control of cardis cds au...      1  train   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0  \\tolivetti does not exclude stake in sgs thoms...  \n",
       "1  \\tu k firm ups italy fund ita stake to pct llo...  \n",
       "2  \\tmoniterm corp mtrm th qtr loss shr loss cts ...  \n",
       "3  \\twal mart stores inc wmt raises quarterly qtl...  \n",
       "4  \\tautospa lube to buy control of cardis cds au...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data into a DataFrame\n",
    "data = pd.read_pickle('../data/newsgroups.pkl')\n",
    "\n",
    "# Definne a simple convenience function for cleaning the strings\n",
    "def clean_text(text):\n",
    "    return \"\".join([c for c in text.lower() if c not in punctuation])\n",
    "\n",
    "# Clean the string labels\n",
    "data['text_cleaned'] = data['text'].map(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_split = data['text_cleaned'].map(lambda x: x.split())\n",
    "all_words = set(list(itertools.chain.from_iterable(text_split)))\n",
    "vocab_size = len(all_words)\n",
    "word2idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "data['idx_encoded'] = data['text_cleaned'].map(lambda x: [word2idx[word] for word in x.split()])\n",
    "\n",
    "labels = data['label']\n",
    "features = data['idx_encoded']\n",
    "\n",
    "labels = data['label'].values\n",
    "features = data['idx_encoded']\n",
    "train_data, test_data = train_test_split(list(zip(features, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = data['label'].values\n",
    "features = data['idx_encoded']\n",
    "train_data, test_data = train_test_split(list(zip(features, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7674, 8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rnn_classifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
    "        super(rnn_classifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax()\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)\n",
    "        e = e.view(len(x), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        output = self.linear(out[-1])\n",
    "        so = self.softmax(output)\n",
    "        return so\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacharybrown/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.98, Validation Accuracy: 0.78\n",
      "Loss: 0.60, Validation Accuracy: 0.82\n",
      "Loss: 0.44, Validation Accuracy: 0.85\n",
      "Loss: 0.35, Validation Accuracy: 0.87\n",
      "Loss: 0.27, Validation Accuracy: 0.88\n",
      "Loss: 0.19, Validation Accuracy: 0.90\n",
      "Loss: 0.15, Validation Accuracy: 0.91\n",
      "Loss: 0.11, Validation Accuracy: 0.90\n",
      "Loss: 0.08, Validation Accuracy: 0.91\n",
      "Loss: 0.07, Validation Accuracy: 0.91\n"
     ]
    }
   ],
   "source": [
    "model = rnn_classifier(vocab_size = vocab_size, \n",
    "                       embedding_dim=100, \n",
    "                       hidden_dim=50, \n",
    "                       output_dim=8, \n",
    "                       batch_size=1)\n",
    "\n",
    "optim = SGD(params=model.parameters(), lr=0.01)\n",
    "criterion = NLLLoss()\n",
    "\n",
    "for i in range(10):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    for it, example in enumerate(train_data):\n",
    "\n",
    "        f, t = example\n",
    "        X = torch.LongTensor(f[:32])\n",
    "        y = torch.LongTensor([t])\n",
    "        \n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model.forward(X)\n",
    "        optim.zero_grad()\n",
    "        prediction = torch.argmax(output)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.data.numpy()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for example in test_data:\n",
    "        optim.zero_grad()\n",
    "        f, t = example\n",
    "        X = torch.LongTensor(f[:32])\n",
    "        y = torch.LongTensor([t])\n",
    "\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model.forward(X)\n",
    "        prediction = torch.argmax(output)\n",
    "\n",
    "        y_true.append(y.data.numpy()[0])\n",
    "        y_pred.append(torch.argmax(output.data).numpy())\n",
    "\n",
    "        a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    total_loss /= (it + 1)\n",
    "\n",
    "    print(\"Loss: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5880, -0.8107]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6805, -0.7059]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
