{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.nn import Linear, Embedding, RNN, GRU, LSTM\n",
    "from torch.nn import Sigmoid, LogSoftmax, Softmax\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss, NLLLoss, CrossEntropyLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pierre, vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NOUN, NOUN, ., NUM, NOUN, ADJ, ., VERB, VERB,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[mr., vinken, is, chairman, of, elsevier, n.v....</td>\n",
       "      <td>[NOUN, NOUN, VERB, NOUN, ADP, NOUN, NOUN, ., D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[rudolph, agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NOUN, NOUN, ., NUM, NOUN, ADJ, CONJ, ADJ, NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[a, form, of, asbestos, once, used, *, *, to, ...</td>\n",
       "      <td>[DET, NOUN, ADP, NOUN, ADV, VERB, X, X, PRT, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DET, NOUN, NOUN, ., NOUN, ., VERB, ADV, ADJ, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  [pierre, vinken, ,, 61, years, old, ,, will, j...   \n",
       "1  [mr., vinken, is, chairman, of, elsevier, n.v....   \n",
       "2  [rudolph, agnew, ,, 55, years, old, and, forme...   \n",
       "3  [a, form, of, asbestos, once, used, *, *, to, ...   \n",
       "4  [the, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "\n",
       "                                               label  \n",
       "0  [NOUN, NOUN, ., NUM, NOUN, ADJ, ., VERB, VERB,...  \n",
       "1  [NOUN, NOUN, VERB, NOUN, ADP, NOUN, NOUN, ., D...  \n",
       "2  [NOUN, NOUN, ., NUM, NOUN, ADJ, CONJ, ADJ, NOU...  \n",
       "3  [DET, NOUN, ADP, NOUN, ADV, VERB, X, X, PRT, V...  \n",
       "4  [DET, NOUN, NOUN, ., NOUN, ., VERB, ADV, ADJ, ...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../data/penn_treebank_pos.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set(itertools.chain.from_iterable(data['text']))\n",
    "all_labels = set(itertools.chain.from_iterable(data['label']))\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "label2idx = {word: idx for idx, word in enumerate(all_labels)}\n",
    "idx2label = {idx: word for word, idx in label2idx.items()}\n",
    "\n",
    "vocab_size = len(all_words)\n",
    "label_size = len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data['text'].map(lambda x: [word2idx[i] for i in x]).tolist()\n",
    "labels = data['label'].map(lambda x: [label2idx[i] for i in x]).tolist()\n",
    "\n",
    "train_data, test_data = train_test_split(list(zip(features, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pos_tagger(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
    "        super(pos_tagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax(dim=2)\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)\n",
    "        e = e.view(len(x), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        output = self.linear(out)\n",
    "        so = self.softmax(output)\n",
    "        return so\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pos_tagger(vocab_size = vocab_size, \n",
    "                       embedding_dim=100, \n",
    "                       hidden_dim=50, \n",
    "                       output_dim=label_size, \n",
    "                       batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.LongTensor(features[0])\n",
    "y = torch.LongTensor(labels[0])\n",
    "\n",
    "output = model.forward(X).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4618, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = NLLLoss()\n",
    "\n",
    "criterion(output, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.17, Training Accuracy: 0.66, Validation Accuracy: 0.70\n",
      "Loss: 0.95, Training Accuracy: 0.72, Validation Accuracy: 0.74\n",
      "Loss: 0.81, Training Accuracy: 0.76, Validation Accuracy: 0.77\n",
      "Loss: 0.72, Training Accuracy: 0.78, Validation Accuracy: 0.79\n",
      "Loss: 0.66, Training Accuracy: 0.80, Validation Accuracy: 0.81\n",
      "Loss: 0.60, Training Accuracy: 0.82, Validation Accuracy: 0.83\n",
      "Loss: 0.56, Training Accuracy: 0.83, Validation Accuracy: 0.84\n",
      "Loss: 0.52, Training Accuracy: 0.84, Validation Accuracy: 0.84\n",
      "Loss: 0.49, Training Accuracy: 0.85, Validation Accuracy: 0.85\n",
      "Loss: 0.46, Training Accuracy: 0.86, Validation Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "optim = SGD(params=model.parameters(), lr=0.01)\n",
    "criterion = NLLLoss()\n",
    "\n",
    "for i in range(10):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    y_true_train = []\n",
    "    y_pred_train = []\n",
    "    for it, example in enumerate(train_data):\n",
    "\n",
    "        f, t = example\n",
    "        X = torch.LongTensor(f)\n",
    "        y = torch.LongTensor(t)\n",
    "        \n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model.forward(X).squeeze(1)\n",
    "        optim.zero_grad()\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.data.numpy()\n",
    "        y_true_train.extend(list(y.data.numpy()))\n",
    "        y_pred_train.extend(list(prediction.numpy()))\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for example in test_data:\n",
    "        optim.zero_grad()\n",
    "        f, t = example\n",
    "        X = torch.LongTensor(f)\n",
    "        y = torch.LongTensor(t)\n",
    "\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model.forward(X).squeeze(1)\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "\n",
    "        y_true.extend(list(y.data.numpy()))\n",
    "        y_pred.extend(list(prediction.numpy()))\n",
    "\n",
    "    a = accuracy_score(y_true, y_pred)\n",
    "    a_train = accuracy_score(y_true_train, y_pred_train)\n",
    "    total_loss /= (it + 1)\n",
    "\n",
    "    print(\"Loss: {:.2f}, Training Accuracy: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a_train, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we PRON\n",
      "run VERB\n",
      "home NOUN\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "sentence = \"we run home .\"\n",
    "words = sentence.lower().split()\n",
    "sample = [word2idx[i] for i in words]\n",
    "preds = [idx2label[i] for i in list(torch.argmax(model.forward(torch.LongTensor(sample)), dim=2).data.numpy().reshape(-1))]\n",
    "for word, pred in zip(words, preds):\n",
    "    print(word, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i PRON\n",
      "went VERB\n",
      "for ADP\n",
      "a DET\n",
      "run NOUN\n",
      "today NOUN\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I went for a run today\"\n",
    "words = sentence.lower().split()\n",
    "sample = [word2idx[i] for i in words]\n",
    "preds = [idx2label[i] for i in list(torch.argmax(model.forward(torch.LongTensor(sample)), dim=2).data.numpy().reshape(-1))]\n",
    "for word, pred in zip(words, preds):\n",
    "    print(word, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"we run home .\"\n",
    "words = sentence.lower().split()\n",
    "sample = [word2idx[i] for i in words]\n",
    "preds = [idx2label[i] for i in list(torch.argmax(model.forward(torch.LongTensor(sample)), dim=2).data.numpy().reshape(-1))]\n",
    "for word, pred in zip(words, preds):\n",
    "    print(word, pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
