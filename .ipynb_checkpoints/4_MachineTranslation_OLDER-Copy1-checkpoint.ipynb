{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.nn import Linear, Embedding, RNN, GRU, LSTM\n",
    "from torch.nn import Sigmoid, LogSoftmax\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import BCELoss, NLLLoss, CrossEntropyLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11618\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600702</th>\n",
       "      <td>Allow us to state that the opening of the inqu...</td>\n",
       "      <td>Permita que le digamos que las investigaciones...</td>\n",
       "      <td>[allow, us, to, state, that, the, opening, of,...</td>\n",
       "      <td>[permita, que, le, digamos, que, las, investig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>Since June of last year, the OLAF regulation h...</td>\n",
       "      <td>Desde junio del año pasado el Reglamento de la...</td>\n",
       "      <td>[since, june, of, last, year,, the, olaf, regu...</td>\n",
       "      <td>[desde, junio, del, año, pasado, el, reglament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180999</th>\n",
       "      <td>I am fully aware of the views expressed here o...</td>\n",
       "      <td>Soy plenamente consciente de las opiniones exp...</td>\n",
       "      <td>[i, am, fully, aware, of, the, views, expresse...</td>\n",
       "      <td>[soy, plenamente, consciente, de, las, opinion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109251</th>\n",
       "      <td>I will have clarification of that situation la...</td>\n",
       "      <td>Hoy mismo recibiré información sobre dicha sit...</td>\n",
       "      <td>[i, will, have, clarification, of, that, situa...</td>\n",
       "      <td>[hoy, mismo, recibiré, información, sobre, dic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401751</th>\n",
       "      <td>Let us not forget, the Roma were the first to ...</td>\n",
       "      <td>No olvidemos que la población gitana fue la pr...</td>\n",
       "      <td>[let, us, not, forget,, the, roma, were, the, ...</td>\n",
       "      <td>[no, olvidemos, que, la, población, gitana, fu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   english  \\\n",
       "600702   Allow us to state that the opening of the inqu...   \n",
       "3733     Since June of last year, the OLAF regulation h...   \n",
       "1180999  I am fully aware of the views expressed here o...   \n",
       "109251   I will have clarification of that situation la...   \n",
       "1401751  Let us not forget, the Roma were the first to ...   \n",
       "\n",
       "                                                   spanish  \\\n",
       "600702   Permita que le digamos que las investigaciones...   \n",
       "3733     Desde junio del año pasado el Reglamento de la...   \n",
       "1180999  Soy plenamente consciente de las opiniones exp...   \n",
       "109251   Hoy mismo recibiré información sobre dicha sit...   \n",
       "1401751  No olvidemos que la población gitana fue la pr...   \n",
       "\n",
       "                                                      text  \\\n",
       "600702   [allow, us, to, state, that, the, opening, of,...   \n",
       "3733     [since, june, of, last, year,, the, olaf, regu...   \n",
       "1180999  [i, am, fully, aware, of, the, views, expresse...   \n",
       "109251   [i, will, have, clarification, of, that, situa...   \n",
       "1401751  [let, us, not, forget,, the, roma, were, the, ...   \n",
       "\n",
       "                                                     label  \n",
       "600702   [permita, que, le, digamos, que, las, investig...  \n",
       "3733     [desde, junio, del, año, pasado, el, reglament...  \n",
       "1180999  [soy, plenamente, consciente, de, las, opinion...  \n",
       "109251   [hoy, mismo, recibiré, información, sobre, dic...  \n",
       "1401751  [no, olvidemos, que, la, población, gitana, fu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../data/4_europarl_en_sp.pkl')\n",
    "\n",
    "print(data.shape[0])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600702</th>\n",
       "      <td>Allow us to state that the opening of the inqu...</td>\n",
       "      <td>Permita que le digamos que las investigaciones...</td>\n",
       "      <td>[&lt;SOS&gt;, allow, us, to, state, that, the, openi...</td>\n",
       "      <td>[&lt;SOS&gt;, permita, que, le, digamos, que, las, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>Since June of last year, the OLAF regulation h...</td>\n",
       "      <td>Desde junio del año pasado el Reglamento de la...</td>\n",
       "      <td>[&lt;SOS&gt;, since, june, of, last, year,, the, ola...</td>\n",
       "      <td>[&lt;SOS&gt;, desde, junio, del, año, pasado, el, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180999</th>\n",
       "      <td>I am fully aware of the views expressed here o...</td>\n",
       "      <td>Soy plenamente consciente de las opiniones exp...</td>\n",
       "      <td>[&lt;SOS&gt;, i, am, fully, aware, of, the, views, e...</td>\n",
       "      <td>[&lt;SOS&gt;, soy, plenamente, consciente, de, las, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109251</th>\n",
       "      <td>I will have clarification of that situation la...</td>\n",
       "      <td>Hoy mismo recibiré información sobre dicha sit...</td>\n",
       "      <td>[&lt;SOS&gt;, i, will, have, clarification, of, that...</td>\n",
       "      <td>[&lt;SOS&gt;, hoy, mismo, recibiré, información, sob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401751</th>\n",
       "      <td>Let us not forget, the Roma were the first to ...</td>\n",
       "      <td>No olvidemos que la población gitana fue la pr...</td>\n",
       "      <td>[&lt;SOS&gt;, let, us, not, forget,, the, roma, were...</td>\n",
       "      <td>[&lt;SOS&gt;, no, olvidemos, que, la, población, git...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   english  \\\n",
       "600702   Allow us to state that the opening of the inqu...   \n",
       "3733     Since June of last year, the OLAF regulation h...   \n",
       "1180999  I am fully aware of the views expressed here o...   \n",
       "109251   I will have clarification of that situation la...   \n",
       "1401751  Let us not forget, the Roma were the first to ...   \n",
       "\n",
       "                                                   spanish  \\\n",
       "600702   Permita que le digamos que las investigaciones...   \n",
       "3733     Desde junio del año pasado el Reglamento de la...   \n",
       "1180999  Soy plenamente consciente de las opiniones exp...   \n",
       "109251   Hoy mismo recibiré información sobre dicha sit...   \n",
       "1401751  No olvidemos que la población gitana fue la pr...   \n",
       "\n",
       "                                                      text  \\\n",
       "600702   [<SOS>, allow, us, to, state, that, the, openi...   \n",
       "3733     [<SOS>, since, june, of, last, year,, the, ola...   \n",
       "1180999  [<SOS>, i, am, fully, aware, of, the, views, e...   \n",
       "109251   [<SOS>, i, will, have, clarification, of, that...   \n",
       "1401751  [<SOS>, let, us, not, forget,, the, roma, were...   \n",
       "\n",
       "                                                     label  \n",
       "600702   [<SOS>, permita, que, le, digamos, que, las, i...  \n",
       "3733     [<SOS>, desde, junio, del, año, pasado, el, re...  \n",
       "1180999  [<SOS>, soy, plenamente, consciente, de, las, ...  \n",
       "109251   [<SOS>, hoy, mismo, recibiré, información, sob...  \n",
       "1401751  [<SOS>, no, olvidemos, que, la, población, git...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].map(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "data['label'] = data['label'].map(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = set(itertools.chain.from_iterable(data['text']))\n",
    "output_words = set(itertools.chain.from_iterable(data['label']))\n",
    "\n",
    "input2idx = {word: idx for idx, word in enumerate(input_words)}\n",
    "idx2input = {idx: word for word, idx in input2idx.items()}\n",
    "\n",
    "output2idx = {word: idx for idx, word in enumerate(output_words)}\n",
    "idx2putput = {idx: word for word, idx in output2idx.items()}\n",
    "\n",
    "input_size = len(input_words)\n",
    "output_size = len(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21842"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_indices = {\n",
    "                        'input2idx': input2idx, \n",
    "                        'idx2input': idx2input, \n",
    "                        'output2idx': output2idx, \n",
    "                        'idx2putput': idx2putput\n",
    "                      }\n",
    "\n",
    "with open('../data/translation_indices.pkl', 'wb') as f:\n",
    "    pickle.dump(translation_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = data['text'].map(lambda x: [input2idx[i] for i in x]).tolist()\n",
    "output_seqs = data['label'].map(lambda x: [output2idx[i] for i in x]).tolist()\n",
    "\n",
    "data = list(zip(input_seqs, output_seqs))\n",
    "\n",
    "train_data, test_data = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax()\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)\n",
    "        e = e.view(len(x), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        return out, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
    "        super(decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax(dim=1)\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, input, hidden):\n",
    "        self.hidden = hidden\n",
    "        e = self.embedding(input)\n",
    "        e = e.view(len(input), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        output = self.linear(out[0])\n",
    "        so = self.softmax(output)\n",
    "        return so, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class seq2seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "                \n",
    "    def forward(self, input_seq, output_seq, p_tf=0):\n",
    "        outputs = []\n",
    "        \n",
    "        self.enc.hidden = self.enc.init_hidden()\n",
    "        self.dec.hidden = self.dec.init_hidden()        \n",
    "        \n",
    "        enc_output, enc_hidden = enc.forward(torch.LongTensor(input_seq))\n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        for i in range(output_seq.shape[0]):\n",
    "            dec_input = torch.LongTensor([output_seq[i]])\n",
    "            dec_output, dec_hidden = self.dec.forward(dec_input, dec_hidden) \n",
    "            outputs.append(dec_output)\n",
    "        #return torch.stack(outputs).squeeze(1)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = input_size\n",
    "enc_embedding_dim = 100\n",
    "enc_hidden_dim = 50\n",
    "\n",
    "dec_vocab_size = output_size\n",
    "dec_embedding_dim = 50\n",
    "dec_hidden_dim = 50\n",
    "dec_output_dim = output_size\n",
    "\n",
    "enc = encoder(enc_vocab_size, enc_embedding_dim, enc_hidden_dim, batch_size=1)\n",
    "dec = decoder(dec_vocab_size, dec_embedding_dim, dec_hidden_dim, dec_output_dim, batch_size=1)\n",
    "s2s = seq2seq(enc, dec)\n",
    "\n",
    "\n",
    "optim = SGD(params=s2s.parameters(), lr=0.01)\n",
    "criterion = NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch|it: 0|100, Total Loss: 9.82\n",
      "9.757656\n",
      "[9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101]\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0002, 0.0002, 0.0001, 0.0001],\n",
      "       grad_fn=<ExpBackward>)\n",
      "Epoch|it: 0|200, Total Loss: 9.79\n",
      "8.636137\n",
      "[9101]\n",
      "tensor([0.0002], grad_fn=<ExpBackward>)\n",
      "Epoch|it: 0|300, Total Loss: 9.78\n",
      "9.729606\n",
      "[9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101]\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001,\n",
      "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002, 0.0002, 0.0002, 0.0002], grad_fn=<ExpBackward>)\n",
      "Epoch|it: 0|400, Total Loss: 9.77\n",
      "9.747605\n",
      "[9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101]\n",
      "tensor([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
      "        0.0002, 0.0002, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002,\n",
      "        0.0002, 0.0002, 0.0002, 0.0002], grad_fn=<ExpBackward>)\n",
      "Epoch|it: 0|500, Total Loss: 9.75\n",
      "9.801451\n",
      "[9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101]\n",
      "tensor([0.0003, 0.0002, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0002, 0.0003],\n",
      "       grad_fn=<ExpBackward>)\n",
      "Epoch|it: 0|600, Total Loss: 9.74\n",
      "9.81456\n",
      "[9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101]\n",
      "tensor([0.0004, 0.0004, 0.0004, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0002, 0.0003, 0.0003, 0.0003,\n",
      "        0.0002, 0.0003, 0.0002, 0.0002, 0.0003, 0.0002, 0.0002, 0.0003],\n",
      "       grad_fn=<ExpBackward>)\n",
      "Epoch|it: 0|700, Total Loss: 9.72\n",
      "9.679267\n",
      "[9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101]\n",
      "tensor([0.0005, 0.0005, 0.0004, 0.0004, 0.0003, 0.0003, 0.0004, 0.0003, 0.0003,\n",
      "        0.0005, 0.0004, 0.0003, 0.0003, 0.0003, 0.0004, 0.0005, 0.0004, 0.0004,\n",
      "        0.0003, 0.0003, 0.0005, 0.0004], grad_fn=<ExpBackward>)\n",
      "Epoch|it: 0|800, Total Loss: 9.71\n",
      "9.6104765\n",
      "[9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101]\n",
      "tensor([0.0006, 0.0007, 0.0007, 0.0006, 0.0006, 0.0006, 0.0005, 0.0004, 0.0004,\n",
      "        0.0006, 0.0006, 0.0005, 0.0005, 0.0004, 0.0004, 0.0005, 0.0007, 0.0005,\n",
      "        0.0005, 0.0004, 0.0004], grad_fn=<ExpBackward>)\n",
      "Epoch|it: 0|900, Total Loss: 9.69\n",
      "9.673867\n",
      "[9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101]\n",
      "tensor([0.0008, 0.0007, 0.0005, 0.0004, 0.0004, 0.0005, 0.0004, 0.0005, 0.0006,\n",
      "        0.0005, 0.0005, 0.0004, 0.0004, 0.0004, 0.0005, 0.0005, 0.0006, 0.0005,\n",
      "        0.0007, 0.0006, 0.0005, 0.0005, 0.0005, 0.0004, 0.0006, 0.0005, 0.0005,\n",
      "        0.0005, 0.0005, 0.0005], grad_fn=<ExpBackward>)\n",
      "Epoch|it: 0|1000, Total Loss: 9.68\n",
      "9.404089\n",
      "[9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101]\n",
      "tensor([0.0010, 0.0008, 0.0005, 0.0009, 0.0006, 0.0007, 0.0009, 0.0008, 0.0007,\n",
      "        0.0008, 0.0008, 0.0006, 0.0007, 0.0007, 0.0006], grad_fn=<ExpBackward>)\n",
      "Epoch|it: 0|1100, Total Loss: 9.66\n",
      "9.675607\n",
      "[9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101, 9101]\n",
      "tensor([0.0014, 0.0010, 0.0007, 0.0005, 0.0008, 0.0007, 0.0008, 0.0011, 0.0009,\n",
      "        0.0009, 0.0008, 0.0009, 0.0009, 0.0008, 0.0009], grad_fn=<ExpBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-477-bab88b53a7e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    s2s.train()\n",
    "    total_loss = 0\n",
    "    s2s.train()\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "    y_train_pred = []\n",
    "    y_train_true = []\n",
    "    for it, example in enumerate(train_data):\n",
    "        if (it % 100 == 0) and (it != 0):\n",
    "            print(\"Epoch|it: {}|{}, Total Loss: {:.2f}\".format(epoch, it, total_loss / it))\n",
    "            print(loss.data.numpy())\n",
    "            print(preds)\n",
    "            print(torch.exp(torch.stack(res_raw).squeeze(1)[:,9101]))\n",
    "        input_seq, output_seq = example\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        \n",
    "        \n",
    "        output_seq = torch.LongTensor(output_seq)    \n",
    "        res_raw = s2s.forward(input_seq, output_seq[:-1])\n",
    "        res = torch.stack(res_raw).squeeze(1)\n",
    "        loss = criterion(res, output_seq[1:])\n",
    "        loss.backward()\n",
    "        total_loss += loss.data.numpy()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "        \n",
    "        preds = list(torch.argmax(res, dim=1).data.numpy())\n",
    "        trues = list(output_seq.data.numpy())\n",
    "        y_train_true.extend(trues)\n",
    "        y_train_pred.extend(preds)\n",
    "        \n",
    "    a_train = accuracy_score(y_train_true, y_train_pred)\n",
    "    \n",
    "    y_test_pred = []\n",
    "    y_test_true = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.7085, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion(res, output_seq[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6969,   914,  9921,  7474,  8462,  5761,  6727, 17023,  9921, 13449,\n",
       "        10371,  6081, 11620,  6314,  3911, 15170,  7101, 21341,  9101])"
      ]
     },
     "execution_count": 482,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_seq[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.398e-05,\n",
       " 4.3541e-05,\n",
       " 4.5462e-05,\n",
       " 5.0658e-05,\n",
       " 4.7762e-05,\n",
       " 5.2381e-05,\n",
       " 5.5144e-05,\n",
       " 5.4351e-05,\n",
       " 4.9266e-05,\n",
       " 5.4104e-05,\n",
       " 5.1706e-05]"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[4.3980e-05, 4.3541e-05, 4.5462e-05, 5.0658e-05, 4.7762e-05, 5.2381e-05,\n",
    "        5.5144e-05, 5.4351e-05, 4.9266e-05, 5.4104e-05, 5.1706e-05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n",
      "<EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "input_seq = torch.LongTensor(input_seqs[3])\n",
    "c, h = s2s.enc.forward(input_seq)\n",
    "\n",
    "output, hidden = s2s.dec.forward(torch.LongTensor([15310]), h)\n",
    "\n",
    "pred = torch.argmax(output)\n",
    "for i in range(30):\n",
    "    pred = torch.argmax(output)\n",
    "    output, hidden = s2s.dec.forward(torch.LongTensor([pred]), hidden)\n",
    "    print(idx2putput[int(pred.numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch|it: 0|500, Total Loss: 9.87\n",
      "Epoch|it: 0|1000, Total Loss: 9.72\n",
      "Epoch|it: 0|1500, Total Loss: 9.49\n",
      "Epoch|it: 0|2000, Total Loss: 9.16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-e106e40e8e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    s2s.train()\n",
    "    total_loss = 0\n",
    "    s2s.train()\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "    y_train_pred = []\n",
    "    y_train_true = []\n",
    "    for it, example in enumerate(train_data):\n",
    "        if (it % 500 == 0) and (it != 0):\n",
    "            print(\"Epoch|it: {}|{}, Total Loss: {:.2f}\".format(epoch, it, total_loss / it))\n",
    "        input_seq, output_seq = example\n",
    "        enc_optim.zero_grad()\n",
    "        dec_optim.zero_grad()\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)    \n",
    "        res = s2s.forward(input_seq, output_seq)\n",
    "        loss = criterion(res, torch.LongTensor(output_seq))\n",
    "        loss.backward()\n",
    "        total_loss += loss.data.numpy()\n",
    "\n",
    "        enc_optim.step()\n",
    "        dec_optim.step()\n",
    "        \n",
    "        preds = list(torch.argmax(res, dim=1).data.numpy())\n",
    "        trues = list(output_seq.data.numpy())\n",
    "        y_train_true.extend(trues)\n",
    "        y_train_pred.extend(preds)\n",
    "        \n",
    "    a_train = accuracy_score(y_train_true, y_train_pred)\n",
    "    \n",
    "    s2s.eval()\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "    \"\"\"\n",
    "    for example in test_data:\n",
    "\n",
    "        input_seq, output_seq = example\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)    \n",
    "        res = s2s.forward(input_seq, output_seq)\n",
    "        preds = list(torch.argmax(res, dim=1).data.numpy())\n",
    "        trues = list(output_seq.data.numpy())\n",
    "\n",
    "        y_test_true.extend(trues)\n",
    "        y_test_pred.extend(preds)\n",
    "\n",
    "    a_test = accuracy_score(y_test_true, y_test_pred)\n",
    "    \n",
    "    print(\"Epoch {} Loss: {:.2f}, Train/Test Accuracy: {:.3}/{:.3f}\".format(epoch, total_loss / it, a_train, a_test))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS>\n",
      "the\n",
      "eu's\n",
      "hypocrisy\n",
      "is\n",
      "abominable:\n",
      "it\n",
      "is\n",
      "criminalising\n",
      "undocumented\n",
      "immigrants.\n",
      "<EOS>\n",
      "<SOS>\n",
      "la\n",
      "hipocresía\n",
      "de\n",
      "la\n",
      "ue\n",
      "es\n",
      "abominable:\n",
      "criminaliza\n",
      "al\n",
      "inmigrante\n",
      "sin\n",
      "papeles.\n",
      "<EOS>\n",
      "[15310, 15310, 15310, 15310, 15310, 15310, 15310, 15310, 15310, 15310, 15310]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "for i in list(input_seq.numpy()):\n",
    "    print(idx2input[i])\n",
    "\n",
    "for i in list(output_seq.numpy()):\n",
    "    print(idx2putput[i])    \n",
    "    \n",
    "break_level = 10\n",
    "\n",
    "model = s2s\n",
    "\n",
    "preds = []\n",
    "\n",
    "model.enc.hidden = model.enc.init_hidden()\n",
    "model.dec.hidden = model.dec.init_hidden()        \n",
    "\n",
    "enc_output, enc_hidden = model.enc(input_seq)\n",
    "context = (enc_output[-1].unsqueeze(1), enc_output[-1].unsqueeze(1))\n",
    "\n",
    "dec_sos = torch.LongTensor([sos_int])\n",
    "\n",
    "dec_output, hidden = model.dec.forward(dec_sos, context)\n",
    "pred = torch.argmax(dec_output[-1])\n",
    "preds.append(int(pred.data.numpy()))\n",
    "\n",
    "it = 0\n",
    "\n",
    "while pred != eos_int:\n",
    "    it += 1\n",
    "    if it > break_level:\n",
    "        break\n",
    "    dec_output, hidden = model.dec.forward(torch.LongTensor([pred]), hidden)\n",
    "    pred = torch.argmax(dec_output[-1])\n",
    "    preds.append(int(pred.data.numpy()))\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(15310)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, hidden = model.dec.forward(torch.LongTensor([15310]), hidden)\n",
    "torch.argmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n",
      "15310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "sample = torch.LongTensor(test_data[0][0])\n",
    "sos_int = output2idx['<SOS>']\n",
    "eos_int = output2idx['<EOS>']\n",
    "preds = predict(s2s, sample, sos_int, eos_int)\n",
    "for p in preds:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9101"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-1afb8400f5eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "s2s.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    s2s.train()\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "    y_train_pred = []\n",
    "    y_train_true = []\n",
    "    for it, example in enumerate(train_data):\n",
    "        if (it % 500 == 0) and (it != 0):\n",
    "            print(\"Epoch|it: {}|{}, Total Loss: {:.2f}\".format(epoch, it, total_loss / it))\n",
    "        input_seq, output_seq = example\n",
    "        enc_optim.zero_grad()\n",
    "        dec_optim.zero_grad()\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)    \n",
    "        res = s2s.forward(input_seq, output_seq)\n",
    "        loss = criterion(res, torch.LongTensor(output_seq))\n",
    "        loss.backward()\n",
    "        total_loss += loss.data.numpy()\n",
    "\n",
    "        enc_optim.step()\n",
    "        dec_optim.step()\n",
    "        \n",
    "        preds = list(torch.argmax(res, dim=1).data.numpy())\n",
    "        trues = list(output_seq.data.numpy())\n",
    "        y_train_true.extend(trues)\n",
    "        y_train_pred.extend(preds)\n",
    "        \n",
    "    a_train = accuracy_score(y_train_true, y_train_pred)\n",
    "    \n",
    "    s2s.eval()\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "\n",
    "    for example in test_data:\n",
    "\n",
    "        input_seq, output_seq = example\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)    \n",
    "        res = s2s.forward(input_seq, output_seq)\n",
    "        preds = list(torch.argmax(res, dim=1).data.numpy())\n",
    "        trues = list(output_seq.data.numpy())\n",
    "\n",
    "        y_test_true.extend(trues)\n",
    "        y_test_pred.extend(preds)\n",
    "\n",
    "    a_test = accuracy_score(y_test_true, y_test_pred)\n",
    "    \n",
    "    print(\"Epoch {} Loss: {:.2f}, Train/Test Accuracy: {:.3}/{:.3f}\".format(epoch, total_loss / it, a_train, a_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type seq2seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.seq2seq'>: it's not the same object as __main__.seq2seq",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-1c2fa72356c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../data/seq2seq.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0mserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_storages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.seq2seq'>: it's not the same object as __main__.seq2seq"
     ]
    }
   ],
   "source": [
    "from modules.seq2seq import encoder, decoder, seq2seq\n",
    "torch.save(s2s, '../data/seq2seq.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
