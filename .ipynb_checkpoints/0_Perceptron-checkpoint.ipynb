{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid, LogSoftmax\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss, NLLLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Dataset Loading and Cleaning\n",
    "\n",
    "We'll begin by loading a prepared version of the [Stanford Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), on which we'll train a binary classifier. \n",
    "\n",
    "This dataset contains 50k highly polarized movie reviews from IMDB, labeled with positive or negative sentiment. \n",
    "\n",
    "We'll perform some minimal preprocessing on the text itself, simply case-normalization and removal of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Not all, but most of this story is Buster bein...</td>\n",
       "      <td>not all but most of this story is buster being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Eric Bogosian's ability to roll from character...</td>\n",
       "      <td>eric bogosians ability to roll from character ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>I am a professional musician who was inspired ...</td>\n",
       "      <td>i am a professional musician who was inspired ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Robin Williams is excellent in this movie and ...</td>\n",
       "      <td>robin williams is excellent in this movie and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>This is a woeful Hollywood remake of a classic...</td>\n",
       "      <td>this is a woeful hollywood remake of a classic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      1  Not all, but most of this story is Buster bein...   \n",
       "1      1  Eric Bogosian's ability to roll from character...   \n",
       "2      1  I am a professional musician who was inspired ...   \n",
       "3      0  Robin Williams is excellent in this movie and ...   \n",
       "4      0  This is a woeful Hollywood remake of a classic...   \n",
       "\n",
       "                                        text_cleaned  \n",
       "0  not all but most of this story is buster being...  \n",
       "1  eric bogosians ability to roll from character ...  \n",
       "2  i am a professional musician who was inspired ...  \n",
       "3  robin williams is excellent in this movie and ...  \n",
       "4  this is a woeful hollywood remake of a classic...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data into a DataFrame\n",
    "data = pd.read_pickle('../data/aclImdb_combined.pkl')\n",
    "\n",
    "# Definne a simple convenience function for cleaning the strings\n",
    "def clean_text(text):\n",
    "    return \"\".join([c for c in text.lower() if c not in punctuation])\n",
    "\n",
    "# Clean the string labels\n",
    "data['text_cleaned'] = data['text'].map(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Text Vectorization\n",
    "\n",
    "Once we have some (somewhat) clean data, we can then vectorize the corpus the standard term frequency, inverse document frequency. For the sake of time, we'll limit the overall input feature space to the top 1k tokens, based on the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in input corpus: 180395\n"
     ]
    }
   ],
   "source": [
    "# Initialize a TfidfVectorizer Object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit the cleaned text\n",
    "tfidf.fit(data['text_cleaned'])\n",
    "\n",
    "# Examine the total number of tokens in the text\n",
    "print(\"Total tokens in input corpus: {}\".format(len(tfidf.vocabulary_)))\n",
    "\n",
    "# Initialize a TfidfVectorizer Object, this time with a max number of features\n",
    "max_features = 1000\n",
    "tfidf = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "# Fit the cleaned text\n",
    "features = tfidf.fit_transform(data['text_cleaned']).todense()\n",
    "labels = data.label.values.reshape(-1,1)\n",
    "\n",
    "# Create tuples of the feature/label pairs, \n",
    "# and perform a stratified train/test split\n",
    "all_data = list(zip(features, labels))\n",
    "train_data, test_data = train_test_split(all_data, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a. Perceptron Classifier\n",
    "\n",
    "For the simplest perceptron, we'll only need a single linear layer as well as a sigmoid transformation to map the output space from our linear layer into the proper probability distribution. \n",
    "\n",
    "Two other things that need to be considered are the choice of loss funciton and the optimization algorithm. We'll use binary cross entropy for the loss function, and stochastic gradient descent for the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear single linear layer, \n",
    "# with input shape of our feature space \n",
    "# and output shape of 1 (binary classification)\n",
    "linear = Linear(max_features, 1, bias=True)\n",
    "\n",
    "# Create a instance of the sigmoid function\n",
    "# so we can normalize our output to the range [0,1]\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "# Binary cross entropy is an appropriate loss function \n",
    "# for this type of problem, and is implemented in the \n",
    "# `BCELoss` class in pytroch\n",
    "criterion = BCELoss()\n",
    "\n",
    "# We'll use basic stochastic gradient descent\n",
    "# to optimize the parameters of our linear layer \n",
    "# (the sigmoid is a transformation with no parameters)\n",
    "optim = SGD(params=linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2b. Training the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37500/37500 [00:07<00:00, 5002.98it/s]\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "for it, example in tqdm(list(enumerate(train_data))):\n",
    "    optim.zero_grad()\n",
    "    f, t = example\n",
    "    X = torch.FloatTensor(f)\n",
    "    y = torch.FloatTensor(t)\n",
    "    X_prime = linear(X)\n",
    "    output = sigmoid(X_prime)\n",
    "    loss = criterion(output.view(-1), y)\n",
    "    total_loss += loss.data.numpy()\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2c. Evaluating the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.55, Validation Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "linear.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "threshold = 0.5\n",
    "\n",
    "for f, t in test_data:\n",
    "    X = torch.FloatTensor(f)\n",
    "    y = torch.FloatTensor([t])\n",
    "    output = sigmoid(linear(X))\n",
    "    y_true.append(y.data.numpy()[0])\n",
    "    y_pred.append(output.data.numpy()[0])\n",
    "\n",
    "y_pred = [int(p >= threshold) for p in y_pred]\n",
    "a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "total_loss /= (it + 1)\n",
    "\n",
    "print(\"Loss: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2d. Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss: 0.61, Validation Accuracy: 0.80\n",
      "Epoch Loss: 0.52, Validation Accuracy: 0.82\n",
      "Epoch Loss: 0.47, Validation Accuracy: 0.83\n",
      "Epoch Loss: 0.44, Validation Accuracy: 0.83\n",
      "Epoch Loss: 0.42, Validation Accuracy: 0.84\n",
      "Epoch Loss: 0.41, Validation Accuracy: 0.84\n",
      "Epoch Loss: 0.39, Validation Accuracy: 0.85\n",
      "Epoch Loss: 0.38, Validation Accuracy: 0.85\n",
      "Epoch Loss: 0.38, Validation Accuracy: 0.85\n",
      "Epoch Loss: 0.37, Validation Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "\n",
    "linear = Linear(max_features, 1, bias=True)\n",
    "sigmoid = Sigmoid()\n",
    "criterion = BCELoss()\n",
    "optim = SGD(params=linear.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    linear.train()\n",
    "    for it, example in list(enumerate(train_data)):\n",
    "        optim.zero_grad()\n",
    "        f, t = example\n",
    "        X = torch.FloatTensor(f)\n",
    "        y = torch.FloatTensor(t)\n",
    "        X_prime = linear(X)\n",
    "        output = sigmoid(X_prime)\n",
    "        loss = criterion(output.view(-1), y)\n",
    "        total_loss += loss.data.numpy()\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "    linear.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    threshold = 0.5\n",
    "    \n",
    "    for f, t in test_data:\n",
    "        X = torch.FloatTensor(f)\n",
    "        y = torch.FloatTensor(t)\n",
    "        output = sigmoid(linear(X))\n",
    "        y_true.append(y.data.numpy()[0])\n",
    "        y_pred.append(output.data.numpy()[0])\n",
    "        \n",
    "    y_pred = [int(p >= threshold) for p in y_pred]\n",
    "    a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "    total_loss /= (it + 1)\n",
    "    print(\"Epoch Loss: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2e. Creating a Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Loss: 0.61, Validation Accuracy: 0.80\n",
      "Epoch Loss: 0.52, Validation Accuracy: 0.82\n",
      "Epoch Loss: 0.47, Validation Accuracy: 0.83\n",
      "Epoch Loss: 0.44, Validation Accuracy: 0.84\n",
      "Epoch Loss: 0.42, Validation Accuracy: 0.84\n",
      "Epoch Loss: 0.41, Validation Accuracy: 0.84\n",
      "Epoch Loss: 0.39, Validation Accuracy: 0.85\n",
      "Epoch Loss: 0.38, Validation Accuracy: 0.85\n",
      "Epoch Loss: 0.38, Validation Accuracy: 0.85\n",
      "Epoch Loss: 0.37, Validation Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "from modules.perceptron import *\n",
    "\n",
    "\n",
    "\n",
    "model = perceptron(max_features)\n",
    "optim = SGD(params=model.parameters(), lr=0.01)\n",
    "criterion = BCELoss()\n",
    "model = train(model, train_data, optim, criterion, epochs=10, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Multi-class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in input corpus: 36873\n"
     ]
    }
   ],
   "source": [
    "# Load the data into a DataFrame\n",
    "data = pd.read_pickle('../data/stackoverflow_gbq.pkl')\n",
    "\n",
    "# Definne a simple convenience function for cleaning the strings\n",
    "def clean_text(text):\n",
    "    return \"\".join([c for c in text.lower() if c not in punctuation])\n",
    "\n",
    "# Clean the string labels\n",
    "data['text_cleaned'] = data['text'].map(clean_text)\n",
    "\n",
    "# Initialize a TfidfVectorizer Object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit the cleaned text\n",
    "tfidf.fit(data['text_cleaned'])\n",
    "\n",
    "# Examine the total number of tokens in the text\n",
    "print(\"Total tokens in input corpus: {}\".format(len(tfidf.vocabulary_)))\n",
    "\n",
    "# Initialize a TfidfVectorizer Object, this time with a max number of features\n",
    "max_features = 1000\n",
    "tfidf = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "# Fit the cleaned text\n",
    "features = tfidf.fit_transform(data['text_cleaned']).todense()\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data.label.values).reshape(-1,1)\n",
    "\n",
    "# Create tuples of the feature/label pairs, \n",
    "# and perform a stratified train/test split\n",
    "all_data = list(zip(features, labels))\n",
    "train_data, test_data = train_test_split(all_data, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear single linear layer, \n",
    "# with input shape of our feature space \n",
    "# and output shape of 1 (binary classification)\n",
    "linear = Linear(max_features, len(lb.classes_), bias=True)\n",
    "\n",
    "# Create a instance of the sigmoid function\n",
    "# so we can normalize our output to the range [0,1]\n",
    "softmax = LogSoftmax()\n",
    "\n",
    "# Binary cross entropy is an appropriate loss function \n",
    "# for this type of problem, and is implemented in the \n",
    "# `BCELoss` class in pytroch\n",
    "criterion = NLLLoss()\n",
    "\n",
    "# We'll use basic stochastic gradient descent\n",
    "# to optimize the parameters of our linear layer \n",
    "# (the sigmoid is a transformation with no parameters)\n",
    "optim = SGD(params=linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/75000 [00:00<?, ?it/s]/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n",
      "100%|██████████| 75000/75000 [00:18<00:00, 4156.37it/s]\n"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "for it, example in tqdm(list(enumerate(train_data))):\n",
    "    optim.zero_grad()\n",
    "    f, t = example\n",
    "    X = torch.FloatTensor(f)\n",
    "    y = torch.LongTensor(t)\n",
    "    X_prime = linear(X)\n",
    "    output = softmax(X_prime)\n",
    "    loss = criterion(output, y)\n",
    "    total_loss += loss.data.numpy()\n",
    "    loss.backward()\n",
    "\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.23, Validation Accuracy: 0.63\n"
     ]
    }
   ],
   "source": [
    "linear.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "threshold = 0.5\n",
    "\n",
    "for f, t in test_data:\n",
    "    X = torch.FloatTensor(f)\n",
    "    y = torch.FloatTensor([t])\n",
    "    output = sigmoid(linear(X))\n",
    "    y_true.append(y.data.numpy()[0])\n",
    "    y_pred.append(torch.argmax(output.data).numpy())\n",
    "\n",
    "\n",
    "a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "total_loss /= (it + 1)\n",
    "\n",
    "print(\"Loss: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/75000 [00:00<?, ?it/s]/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "100%|██████████| 75000/75000 [00:18<00:00, 4113.13it/s]\n",
      "  1%|          | 411/75000 [00:00<00:18, 4108.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.24, Validation Accuracy: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:18<00:00, 4166.37it/s]\n",
      "  1%|          | 800/75000 [00:00<00:18, 4003.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.01, Validation Accuracy: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:18<00:00, 4148.23it/s]\n",
      "  1%|          | 830/75000 [00:00<00:17, 4143.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.93, Validation Accuracy: 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:17<00:00, 4256.68it/s]\n",
      "  1%|          | 849/75000 [00:00<00:17, 4209.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.89, Validation Accuracy: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:17<00:00, 4219.92it/s]\n",
      "  1%|          | 855/75000 [00:00<00:17, 4314.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.86, Validation Accuracy: 0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:18<00:00, 4061.18it/s]\n",
      "  1%|          | 418/75000 [00:00<00:17, 4179.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.84, Validation Accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:17<00:00, 4290.82it/s]\n",
      "  1%|          | 869/75000 [00:00<00:17, 4354.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.83, Validation Accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:17<00:00, 4301.00it/s]\n",
      "  1%|          | 431/75000 [00:00<00:17, 4305.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.82, Validation Accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:17<00:00, 4230.70it/s]\n",
      "  1%|          | 424/75000 [00:00<00:17, 4232.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.81, Validation Accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75000/75000 [00:17<00:00, 4291.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.80, Validation Accuracy: 0.70\n"
     ]
    }
   ],
   "source": [
    "linear = Linear(max_features, len(lb.classes_), bias=True)\n",
    "softmax = LogSoftmax()\n",
    "criterion = NLLLoss()\n",
    "optim = SGD(params=linear.parameters(), lr=0.01)\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    linear.train()\n",
    "    total_loss = 0\n",
    "    for it, example in list(enumerate(train_data)):\n",
    "        optim.zero_grad()\n",
    "        f, t = example\n",
    "        X = torch.FloatTensor(f)\n",
    "        y = torch.LongTensor(t)\n",
    "        X_prime = linear(X)\n",
    "        output = softmax(X_prime)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.data.numpy()\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "    linear.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    threshold = 0.5\n",
    "\n",
    "    for f, t in test_data:\n",
    "        X = torch.FloatTensor(f)\n",
    "        y = torch.FloatTensor([t])\n",
    "        output = sigmoid(linear(X))\n",
    "        y_true.append(y.data.numpy()[0])\n",
    "        y_pred.append(torch.argmax(output.data).numpy())\n",
    "\n",
    "\n",
    "    a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    total_loss /= (it + 1)\n",
    "\n",
    "    print(\"Loss: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.275"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
