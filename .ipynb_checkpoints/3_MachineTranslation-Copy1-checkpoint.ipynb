{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.nn import Linear, Embedding, RNN, GRU, LSTM\n",
    "from torch.nn import Sigmoid, LogSoftmax\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss, NLLLoss, CrossEntropyLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11618\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600702</th>\n",
       "      <td>Allow us to state that the opening of the inqu...</td>\n",
       "      <td>Permita que le digamos que las investigaciones...</td>\n",
       "      <td>[allow, us, to, state, that, the, opening, of,...</td>\n",
       "      <td>[permita, que, le, digamos, que, las, investig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>Since June of last year, the OLAF regulation h...</td>\n",
       "      <td>Desde junio del año pasado el Reglamento de la...</td>\n",
       "      <td>[since, june, of, last, year,, the, olaf, regu...</td>\n",
       "      <td>[desde, junio, del, año, pasado, el, reglament...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180999</th>\n",
       "      <td>I am fully aware of the views expressed here o...</td>\n",
       "      <td>Soy plenamente consciente de las opiniones exp...</td>\n",
       "      <td>[i, am, fully, aware, of, the, views, expresse...</td>\n",
       "      <td>[soy, plenamente, consciente, de, las, opinion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109251</th>\n",
       "      <td>I will have clarification of that situation la...</td>\n",
       "      <td>Hoy mismo recibiré información sobre dicha sit...</td>\n",
       "      <td>[i, will, have, clarification, of, that, situa...</td>\n",
       "      <td>[hoy, mismo, recibiré, información, sobre, dic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401751</th>\n",
       "      <td>Let us not forget, the Roma were the first to ...</td>\n",
       "      <td>No olvidemos que la población gitana fue la pr...</td>\n",
       "      <td>[let, us, not, forget,, the, roma, were, the, ...</td>\n",
       "      <td>[no, olvidemos, que, la, población, gitana, fu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   english  \\\n",
       "600702   Allow us to state that the opening of the inqu...   \n",
       "3733     Since June of last year, the OLAF regulation h...   \n",
       "1180999  I am fully aware of the views expressed here o...   \n",
       "109251   I will have clarification of that situation la...   \n",
       "1401751  Let us not forget, the Roma were the first to ...   \n",
       "\n",
       "                                                   spanish  \\\n",
       "600702   Permita que le digamos que las investigaciones...   \n",
       "3733     Desde junio del año pasado el Reglamento de la...   \n",
       "1180999  Soy plenamente consciente de las opiniones exp...   \n",
       "109251   Hoy mismo recibiré información sobre dicha sit...   \n",
       "1401751  No olvidemos que la población gitana fue la pr...   \n",
       "\n",
       "                                                      text  \\\n",
       "600702   [allow, us, to, state, that, the, opening, of,...   \n",
       "3733     [since, june, of, last, year,, the, olaf, regu...   \n",
       "1180999  [i, am, fully, aware, of, the, views, expresse...   \n",
       "109251   [i, will, have, clarification, of, that, situa...   \n",
       "1401751  [let, us, not, forget,, the, roma, were, the, ...   \n",
       "\n",
       "                                                     label  \n",
       "600702   [permita, que, le, digamos, que, las, investig...  \n",
       "3733     [desde, junio, del, año, pasado, el, reglament...  \n",
       "1180999  [soy, plenamente, consciente, de, las, opinion...  \n",
       "109251   [hoy, mismo, recibiré, información, sobre, dic...  \n",
       "1401751  [no, olvidemos, que, la, población, gitana, fu...  "
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../data/europarl_en_sp.pkl')\n",
    "\n",
    "print(data.shape[0])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600702</th>\n",
       "      <td>Allow us to state that the opening of the inqu...</td>\n",
       "      <td>Permita que le digamos que las investigaciones...</td>\n",
       "      <td>[&lt;SOS&gt;, allow, us, to, state, that, the, openi...</td>\n",
       "      <td>[&lt;SOS&gt;, permita, que, le, digamos, que, las, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>Since June of last year, the OLAF regulation h...</td>\n",
       "      <td>Desde junio del año pasado el Reglamento de la...</td>\n",
       "      <td>[&lt;SOS&gt;, since, june, of, last, year,, the, ola...</td>\n",
       "      <td>[&lt;SOS&gt;, desde, junio, del, año, pasado, el, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180999</th>\n",
       "      <td>I am fully aware of the views expressed here o...</td>\n",
       "      <td>Soy plenamente consciente de las opiniones exp...</td>\n",
       "      <td>[&lt;SOS&gt;, i, am, fully, aware, of, the, views, e...</td>\n",
       "      <td>[&lt;SOS&gt;, soy, plenamente, consciente, de, las, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109251</th>\n",
       "      <td>I will have clarification of that situation la...</td>\n",
       "      <td>Hoy mismo recibiré información sobre dicha sit...</td>\n",
       "      <td>[&lt;SOS&gt;, i, will, have, clarification, of, that...</td>\n",
       "      <td>[&lt;SOS&gt;, hoy, mismo, recibiré, información, sob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401751</th>\n",
       "      <td>Let us not forget, the Roma were the first to ...</td>\n",
       "      <td>No olvidemos que la población gitana fue la pr...</td>\n",
       "      <td>[&lt;SOS&gt;, let, us, not, forget,, the, roma, were...</td>\n",
       "      <td>[&lt;SOS&gt;, no, olvidemos, que, la, población, git...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   english  \\\n",
       "600702   Allow us to state that the opening of the inqu...   \n",
       "3733     Since June of last year, the OLAF regulation h...   \n",
       "1180999  I am fully aware of the views expressed here o...   \n",
       "109251   I will have clarification of that situation la...   \n",
       "1401751  Let us not forget, the Roma were the first to ...   \n",
       "\n",
       "                                                   spanish  \\\n",
       "600702   Permita que le digamos que las investigaciones...   \n",
       "3733     Desde junio del año pasado el Reglamento de la...   \n",
       "1180999  Soy plenamente consciente de las opiniones exp...   \n",
       "109251   Hoy mismo recibiré información sobre dicha sit...   \n",
       "1401751  No olvidemos que la población gitana fue la pr...   \n",
       "\n",
       "                                                      text  \\\n",
       "600702   [<SOS>, allow, us, to, state, that, the, openi...   \n",
       "3733     [<SOS>, since, june, of, last, year,, the, ola...   \n",
       "1180999  [<SOS>, i, am, fully, aware, of, the, views, e...   \n",
       "109251   [<SOS>, i, will, have, clarification, of, that...   \n",
       "1401751  [<SOS>, let, us, not, forget,, the, roma, were...   \n",
       "\n",
       "                                                     label  \n",
       "600702   [<SOS>, permita, que, le, digamos, que, las, i...  \n",
       "3733     [<SOS>, desde, junio, del, año, pasado, el, re...  \n",
       "1180999  [<SOS>, soy, plenamente, consciente, de, las, ...  \n",
       "109251   [<SOS>, hoy, mismo, recibiré, información, sob...  \n",
       "1401751  [<SOS>, no, olvidemos, que, la, población, git...  "
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = data['text'].map(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "data['label'] = data['label'].map(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1e+03 ns, total: 5 µs\n",
      "Wall time: 69.1 µs\n"
     ]
    }
   ],
   "source": [
    "input_words = set(itertools.chain.from_iterable(data['text']))\n",
    "output_words = set(itertools.chain.from_iterable(data['label']))\n",
    "\n",
    "input2idx = {word: idx for idx, word in enumerate(input_words)}\n",
    "idx2input = {idx: word for word, idx in input2idx.items()}\n",
    "\n",
    "output2idx = {word: idx for idx, word in enumerate(output_words)}\n",
    "idx2putput = {idx: word for word, idx in output2idx.items()}\n",
    "\n",
    "input_size = len(input_words)\n",
    "output_size = len(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21842"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_indices = {\n",
    "                        'input2idx': input2idx, \n",
    "                        'idx2input': idx2input, \n",
    "                        'output2idx': output2idx, \n",
    "                        'idx2putput': idx2putput\n",
    "                      }\n",
    "\n",
    "with open('../data/translation_indices.pkl', 'wb') as f:\n",
    "    pickle.dump(translation_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = data['text'].map(lambda x: [input2idx[i] for i in x]).tolist()\n",
    "output_seqs = data['label'].map(lambda x: [output2idx[i] for i in x]).tolist()\n",
    "\n",
    "data = list(zip(input_seqs, output_seqs))\n",
    "\n",
    "train_data, test_data = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax()\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)\n",
    "        e = e.view(len(x), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        return out, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
    "        super(decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax()\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, input, hidden):\n",
    "        self.hidden = hidden\n",
    "        e = self.embedding(input)\n",
    "        e = e.view(len(input), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        output = self.linear(out[0])\n",
    "        so = self.softmax(output)\n",
    "        return so, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class seq2seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "                \n",
    "    def forward(self, input_seq, output_seq, p_tf=0):\n",
    "        outputs = []\n",
    "        \n",
    "        enc.hidden = self.enc.init_hidden()\n",
    "        dec.hidden = self.dec.init_hidden()        \n",
    "        \n",
    "        enc_output, enc_hidden = enc(torch.LongTensor(input_seq))\n",
    "        context = enc_output[-1].unsqueeze(0)\n",
    "        \n",
    "        dec_output, hidden = dec(torch.LongTensor([output_seq[0]]), (context, context))\n",
    "        outputs.append(dec_output)\n",
    "        for i in range(1,output_seq.shape[0]):\n",
    "            dec_input = torch.LongTensor([output_seq[i]])\n",
    "            dec_output, hidden = dec(dec_input, hidden) \n",
    "            outputs.append(dec_output)\n",
    "        return torch.stack(outputs).squeeze(1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = input_size\n",
    "enc_embedding_dim = 100\n",
    "enc_hidden_dim = 50\n",
    "\n",
    "dec_vocab_size = output_size\n",
    "dec_embedding_dim = 50\n",
    "dec_hidden_dim = 50\n",
    "dec_output_dim = output_size\n",
    "\n",
    "enc = encoder(enc_vocab_size, enc_embedding_dim, enc_hidden_dim, batch_size=1)\n",
    "dec = decoder(dec_vocab_size, dec_embedding_dim, dec_hidden_dim, dec_output_dim, batch_size=1)\n",
    "s2s = seq2seq(enc, dec)\n",
    "\n",
    "\n",
    "enc_optim = SGD(params=enc.parameters(), lr=0.01)\n",
    "dec_optim = SGD(params=dec.parameters(), lr=0.01)\n",
    "criterion = NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch|it: 0|500, Total Loss: 9.22\n",
      "Epoch|it: 0|1000, Total Loss: 8.62\n",
      "Epoch|it: 0|1500, Total Loss: 8.16\n",
      "Epoch|it: 0|2000, Total Loss: 7.84\n",
      "Epoch|it: 0|2500, Total Loss: 7.60\n",
      "Epoch|it: 0|3000, Total Loss: 7.38\n",
      "Epoch|it: 0|3500, Total Loss: 7.21\n",
      "Epoch|it: 0|4000, Total Loss: 7.07\n",
      "Epoch|it: 0|4500, Total Loss: 6.94\n",
      "Epoch|it: 0|5000, Total Loss: 6.83\n",
      "Epoch|it: 0|5500, Total Loss: 6.73\n",
      "Epoch|it: 0|6000, Total Loss: 6.64\n",
      "Epoch|it: 0|6500, Total Loss: 6.56\n",
      "Epoch|it: 0|7000, Total Loss: 6.49\n",
      "Epoch|it: 0|7500, Total Loss: 6.42\n",
      "Epoch|it: 0|8000, Total Loss: 6.36\n",
      "Epoch|it: 0|8500, Total Loss: 6.30\n",
      "Epoch 0 Loss: 6.27, Train/Test Accuracy: 0.344/0.445\n",
      "Epoch|it: 1|500, Total Loss: 5.36\n",
      "Epoch|it: 1|1000, Total Loss: 5.31\n",
      "Epoch|it: 1|1500, Total Loss: 5.27\n",
      "Epoch|it: 1|2000, Total Loss: 5.26\n",
      "Epoch|it: 1|2500, Total Loss: 5.23\n",
      "Epoch|it: 1|3000, Total Loss: 5.20\n",
      "Epoch|it: 1|3500, Total Loss: 5.19\n",
      "Epoch|it: 1|4000, Total Loss: 5.17\n",
      "Epoch|it: 1|4500, Total Loss: 5.14\n",
      "Epoch|it: 1|5000, Total Loss: 5.13\n",
      "Epoch|it: 1|5500, Total Loss: 5.11\n",
      "Epoch|it: 1|6000, Total Loss: 5.09\n",
      "Epoch|it: 1|6500, Total Loss: 5.08\n",
      "Epoch|it: 1|7000, Total Loss: 5.07\n",
      "Epoch|it: 1|7500, Total Loss: 5.05\n",
      "Epoch|it: 1|8000, Total Loss: 5.04\n",
      "Epoch|it: 1|8500, Total Loss: 5.03\n",
      "Epoch 1 Loss: 5.02, Train/Test Accuracy: 0.466/0.483\n",
      "Epoch|it: 2|500, Total Loss: 4.84\n",
      "Epoch|it: 2|1000, Total Loss: 4.81\n",
      "Epoch|it: 2|1500, Total Loss: 4.78\n",
      "Epoch|it: 2|2000, Total Loss: 4.77\n",
      "Epoch|it: 2|2500, Total Loss: 4.75\n",
      "Epoch|it: 2|3000, Total Loss: 4.73\n",
      "Epoch|it: 2|3500, Total Loss: 4.72\n",
      "Epoch|it: 2|4000, Total Loss: 4.71\n",
      "Epoch|it: 2|4500, Total Loss: 4.69\n",
      "Epoch|it: 2|5000, Total Loss: 4.69\n",
      "Epoch|it: 2|5500, Total Loss: 4.67\n",
      "Epoch|it: 2|6000, Total Loss: 4.66\n",
      "Epoch|it: 2|6500, Total Loss: 4.66\n",
      "Epoch|it: 2|7000, Total Loss: 4.65\n",
      "Epoch|it: 2|7500, Total Loss: 4.64\n",
      "Epoch|it: 2|8000, Total Loss: 4.64\n",
      "Epoch|it: 2|8500, Total Loss: 4.63\n",
      "Epoch 2 Loss: 4.63, Train/Test Accuracy: 0.499/0.514\n",
      "Epoch|it: 3|500, Total Loss: 4.54\n",
      "Epoch|it: 3|1000, Total Loss: 4.51\n",
      "Epoch|it: 3|1500, Total Loss: 4.48\n",
      "Epoch|it: 3|2000, Total Loss: 4.48\n",
      "Epoch|it: 3|2500, Total Loss: 4.46\n",
      "Epoch|it: 3|3000, Total Loss: 4.44\n",
      "Epoch|it: 3|3500, Total Loss: 4.44\n",
      "Epoch|it: 3|4000, Total Loss: 4.43\n",
      "Epoch|it: 3|4500, Total Loss: 4.41\n",
      "Epoch|it: 3|5000, Total Loss: 4.41\n",
      "Epoch|it: 3|5500, Total Loss: 4.40\n",
      "Epoch|it: 3|6000, Total Loss: 4.39\n",
      "Epoch|it: 3|6500, Total Loss: 4.39\n",
      "Epoch|it: 3|7000, Total Loss: 4.39\n",
      "Epoch|it: 3|7500, Total Loss: 4.38\n",
      "Epoch|it: 3|8000, Total Loss: 4.38\n",
      "Epoch|it: 3|8500, Total Loss: 4.37\n",
      "Epoch 3 Loss: 4.37, Train/Test Accuracy: 0.527/0.538\n",
      "Epoch|it: 4|500, Total Loss: 4.31\n",
      "Epoch|it: 4|1000, Total Loss: 4.29\n",
      "Epoch|it: 4|1500, Total Loss: 4.26\n",
      "Epoch|it: 4|2000, Total Loss: 4.26\n",
      "Epoch|it: 4|2500, Total Loss: 4.25\n",
      "Epoch|it: 4|3000, Total Loss: 4.23\n",
      "Epoch|it: 4|3500, Total Loss: 4.23\n",
      "Epoch|it: 4|4000, Total Loss: 4.22\n",
      "Epoch|it: 4|4500, Total Loss: 4.21\n",
      "Epoch|it: 4|5000, Total Loss: 4.21\n",
      "Epoch|it: 4|5500, Total Loss: 4.20\n",
      "Epoch|it: 4|6000, Total Loss: 4.19\n",
      "Epoch|it: 4|6500, Total Loss: 4.19\n",
      "Epoch|it: 4|7000, Total Loss: 4.19\n",
      "Epoch|it: 4|7500, Total Loss: 4.18\n",
      "Epoch|it: 4|8000, Total Loss: 4.18\n",
      "Epoch|it: 4|8500, Total Loss: 4.18\n",
      "Epoch 4 Loss: 4.17, Train/Test Accuracy: 0.548/0.559\n",
      "Epoch|it: 5|500, Total Loss: 4.14\n",
      "Epoch|it: 5|1000, Total Loss: 4.12\n",
      "Epoch|it: 5|1500, Total Loss: 4.09\n",
      "Epoch|it: 5|2000, Total Loss: 4.09\n",
      "Epoch|it: 5|2500, Total Loss: 4.08\n",
      "Epoch|it: 5|3000, Total Loss: 4.06\n",
      "Epoch|it: 5|3500, Total Loss: 4.06\n",
      "Epoch|it: 5|4000, Total Loss: 4.05\n",
      "Epoch|it: 5|4500, Total Loss: 4.04\n",
      "Epoch|it: 5|5000, Total Loss: 4.04\n",
      "Epoch|it: 5|5500, Total Loss: 4.03\n",
      "Epoch|it: 5|6000, Total Loss: 4.03\n",
      "Epoch|it: 5|6500, Total Loss: 4.03\n",
      "Epoch|it: 5|7000, Total Loss: 4.02\n",
      "Epoch|it: 5|7500, Total Loss: 4.02\n",
      "Epoch|it: 5|8000, Total Loss: 4.02\n",
      "Epoch|it: 5|8500, Total Loss: 4.02\n",
      "Epoch 5 Loss: 4.01, Train/Test Accuracy: 0.566/0.573\n",
      "Epoch|it: 6|500, Total Loss: 3.99\n",
      "Epoch|it: 6|1000, Total Loss: 3.98\n",
      "Epoch|it: 6|1500, Total Loss: 3.95\n",
      "Epoch|it: 6|2000, Total Loss: 3.95\n",
      "Epoch|it: 6|2500, Total Loss: 3.93\n",
      "Epoch|it: 6|3000, Total Loss: 3.92\n",
      "Epoch|it: 6|3500, Total Loss: 3.92\n",
      "Epoch|it: 6|4000, Total Loss: 3.91\n",
      "Epoch|it: 6|4500, Total Loss: 3.90\n",
      "Epoch|it: 6|5000, Total Loss: 3.90\n",
      "Epoch|it: 6|5500, Total Loss: 3.89\n",
      "Epoch|it: 6|6000, Total Loss: 3.89\n",
      "Epoch|it: 6|6500, Total Loss: 3.89\n",
      "Epoch|it: 6|7000, Total Loss: 3.89\n",
      "Epoch|it: 6|7500, Total Loss: 3.88\n",
      "Epoch|it: 6|8000, Total Loss: 3.89\n",
      "Epoch|it: 6|8500, Total Loss: 3.88\n",
      "Epoch 6 Loss: 3.88, Train/Test Accuracy: 0.579/0.586\n",
      "Epoch|it: 7|500, Total Loss: 3.86\n",
      "Epoch|it: 7|1000, Total Loss: 3.85\n",
      "Epoch|it: 7|1500, Total Loss: 3.82\n",
      "Epoch|it: 7|2000, Total Loss: 3.82\n",
      "Epoch|it: 7|2500, Total Loss: 3.81\n",
      "Epoch|it: 7|3000, Total Loss: 3.80\n",
      "Epoch|it: 7|3500, Total Loss: 3.80\n",
      "Epoch|it: 7|4000, Total Loss: 3.79\n",
      "Epoch|it: 7|4500, Total Loss: 3.78\n",
      "Epoch|it: 7|5000, Total Loss: 3.78\n",
      "Epoch|it: 7|5500, Total Loss: 3.78\n",
      "Epoch|it: 7|6000, Total Loss: 3.77\n",
      "Epoch|it: 7|6500, Total Loss: 3.77\n",
      "Epoch|it: 7|7000, Total Loss: 3.77\n",
      "Epoch|it: 7|7500, Total Loss: 3.77\n",
      "Epoch|it: 7|8000, Total Loss: 3.77\n",
      "Epoch|it: 7|8500, Total Loss: 3.77\n",
      "Epoch 7 Loss: 3.76, Train/Test Accuracy: 0.592/0.597\n",
      "Epoch|it: 8|500, Total Loss: 3.75\n",
      "Epoch|it: 8|1000, Total Loss: 3.74\n",
      "Epoch|it: 8|1500, Total Loss: 3.71\n",
      "Epoch|it: 8|2000, Total Loss: 3.71\n",
      "Epoch|it: 8|2500, Total Loss: 3.70\n",
      "Epoch|it: 8|3000, Total Loss: 3.69\n",
      "Epoch|it: 8|3500, Total Loss: 3.69\n",
      "Epoch|it: 8|4000, Total Loss: 3.69\n",
      "Epoch|it: 8|4500, Total Loss: 3.67\n",
      "Epoch|it: 8|5000, Total Loss: 3.68\n",
      "Epoch|it: 8|5500, Total Loss: 3.67\n",
      "Epoch|it: 8|6000, Total Loss: 3.66\n",
      "Epoch|it: 8|6500, Total Loss: 3.67\n",
      "Epoch|it: 8|7000, Total Loss: 3.67\n",
      "Epoch|it: 8|7500, Total Loss: 3.66\n",
      "Epoch|it: 8|8000, Total Loss: 3.66\n",
      "Epoch|it: 8|8500, Total Loss: 3.66\n",
      "Epoch 8 Loss: 3.66, Train/Test Accuracy: 0.602/0.607\n",
      "Epoch|it: 9|500, Total Loss: 3.65\n",
      "Epoch|it: 9|1000, Total Loss: 3.64\n",
      "Epoch|it: 9|1500, Total Loss: 3.62\n",
      "Epoch|it: 9|2000, Total Loss: 3.62\n",
      "Epoch|it: 9|2500, Total Loss: 3.61\n",
      "Epoch|it: 9|3000, Total Loss: 3.59\n",
      "Epoch|it: 9|3500, Total Loss: 3.60\n",
      "Epoch|it: 9|4000, Total Loss: 3.59\n",
      "Epoch|it: 9|4500, Total Loss: 3.58\n",
      "Epoch|it: 9|5000, Total Loss: 3.58\n",
      "Epoch|it: 9|5500, Total Loss: 3.58\n",
      "Epoch|it: 9|6000, Total Loss: 3.57\n",
      "Epoch|it: 9|6500, Total Loss: 3.57\n",
      "Epoch|it: 9|7000, Total Loss: 3.57\n",
      "Epoch|it: 9|7500, Total Loss: 3.57\n",
      "Epoch|it: 9|8000, Total Loss: 3.57\n",
      "Epoch|it: 9|8500, Total Loss: 3.57\n",
      "Epoch 9 Loss: 3.57, Train/Test Accuracy: 0.612/0.616\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "s2s.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    s2s.train()\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "    y_train_pred = []\n",
    "    y_train_true = []\n",
    "    for it, example in enumerate(train_data):\n",
    "        if (it % 500 == 0) and (it != 0):\n",
    "            print(\"Epoch|it: {}|{}, Total Loss: {:.2f}\".format(epoch, it, total_loss / it))\n",
    "        input_seq, output_seq = example\n",
    "        enc_optim.zero_grad()\n",
    "        dec_optim.zero_grad()\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)    \n",
    "        res = s2s.forward(input_seq, output_seq)\n",
    "        loss = criterion(res, torch.LongTensor(output_seq))\n",
    "        loss.backward()\n",
    "        total_loss += loss.data.numpy()\n",
    "\n",
    "        enc_optim.step()\n",
    "        dec_optim.step()\n",
    "        \n",
    "        preds = list(torch.argmax(res, dim=1).data.numpy())\n",
    "        trues = list(output_seq.data.numpy())\n",
    "        y_train_true.extend(trues)\n",
    "        y_train_pred.extend(preds)\n",
    "        \n",
    "    a_train = accuracy_score(y_train_true, y_train_pred)\n",
    "    \n",
    "    s2s.eval()\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "\n",
    "    for example in test_data:\n",
    "\n",
    "        input_seq, output_seq = example\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)    \n",
    "        res = s2s.forward(input_seq, output_seq)\n",
    "        preds = list(torch.argmax(res, dim=1).data.numpy())\n",
    "        trues = list(output_seq.data.numpy())\n",
    "\n",
    "        y_test_true.extend(trues)\n",
    "        y_test_pred.extend(preds)\n",
    "\n",
    "    a_test = accuracy_score(y_test_true, y_test_pred)\n",
    "    \n",
    "    print(\"Epoch {} Loss: {:.2f}, Train/Test Accuracy: {:.3}/{:.3f}\".format(epoch, total_loss / it, a_train, a_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(s2s.state_dict(), '../data/s2s.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch|it: 0|500, Total Loss: 3.56\n",
      "Epoch|it: 0|1000, Total Loss: 3.55\n",
      "Epoch|it: 0|1500, Total Loss: 3.53\n",
      "Epoch|it: 0|2000, Total Loss: 3.53\n",
      "Epoch|it: 0|2500, Total Loss: 3.52\n",
      "Epoch|it: 0|3000, Total Loss: 3.51\n",
      "Epoch|it: 0|3500, Total Loss: 3.51\n",
      "Epoch|it: 0|4000, Total Loss: 3.50\n",
      "Epoch|it: 0|4500, Total Loss: 3.49\n",
      "Epoch|it: 0|5000, Total Loss: 3.50\n",
      "Epoch|it: 0|5500, Total Loss: 3.49\n",
      "Epoch|it: 0|6000, Total Loss: 3.48\n",
      "Epoch|it: 0|6500, Total Loss: 3.49\n",
      "Epoch|it: 0|7000, Total Loss: 3.49\n",
      "Epoch|it: 0|7500, Total Loss: 3.48\n",
      "Epoch|it: 0|8000, Total Loss: 3.49\n",
      "Epoch|it: 0|8500, Total Loss: 3.48\n",
      "Epoch 0 Loss: 3.48, Train/Test Accuracy: 0.622/0.627\n",
      "Epoch|it: 1|500, Total Loss: 3.48\n",
      "Epoch|it: 1|1000, Total Loss: 3.47\n",
      "Epoch|it: 1|1500, Total Loss: 3.45\n",
      "Epoch|it: 1|2000, Total Loss: 3.45\n",
      "Epoch|it: 1|2500, Total Loss: 3.44\n",
      "Epoch|it: 1|3000, Total Loss: 3.43\n",
      "Epoch|it: 1|3500, Total Loss: 3.43\n",
      "Epoch|it: 1|4000, Total Loss: 3.43\n",
      "Epoch|it: 1|4500, Total Loss: 3.41\n",
      "Epoch|it: 1|5000, Total Loss: 3.42\n",
      "Epoch|it: 1|5500, Total Loss: 3.41\n",
      "Epoch|it: 1|6000, Total Loss: 3.41\n",
      "Epoch|it: 1|6500, Total Loss: 3.41\n",
      "Epoch|it: 1|7000, Total Loss: 3.41\n",
      "Epoch|it: 1|7500, Total Loss: 3.40\n",
      "Epoch|it: 1|8000, Total Loss: 3.41\n",
      "Epoch|it: 1|8500, Total Loss: 3.41\n",
      "Epoch 1 Loss: 3.40, Train/Test Accuracy: 0.632/0.635\n",
      "Epoch|it: 2|500, Total Loss: 3.40\n",
      "Epoch|it: 2|1000, Total Loss: 3.40\n",
      "Epoch|it: 2|1500, Total Loss: 3.37\n",
      "Epoch|it: 2|2000, Total Loss: 3.37\n",
      "Epoch|it: 2|2500, Total Loss: 3.36\n",
      "Epoch|it: 2|3000, Total Loss: 3.35\n",
      "Epoch|it: 2|3500, Total Loss: 3.36\n",
      "Epoch|it: 2|4000, Total Loss: 3.35\n",
      "Epoch|it: 2|4500, Total Loss: 3.34\n",
      "Epoch|it: 2|5000, Total Loss: 3.34\n",
      "Epoch|it: 2|5500, Total Loss: 3.34\n",
      "Epoch|it: 2|6000, Total Loss: 3.33\n",
      "Epoch|it: 2|6500, Total Loss: 3.34\n",
      "Epoch|it: 2|7000, Total Loss: 3.34\n",
      "Epoch|it: 2|7500, Total Loss: 3.33\n",
      "Epoch|it: 2|8000, Total Loss: 3.34\n",
      "Epoch|it: 2|8500, Total Loss: 3.33\n",
      "Epoch 2 Loss: 3.33, Train/Test Accuracy: 0.638/0.640\n",
      "Epoch|it: 3|500, Total Loss: 3.33\n",
      "Epoch|it: 3|1000, Total Loss: 3.33\n",
      "Epoch|it: 3|1500, Total Loss: 3.30\n",
      "Epoch|it: 3|2000, Total Loss: 3.30\n",
      "Epoch|it: 3|2500, Total Loss: 3.29\n",
      "Epoch|it: 3|3000, Total Loss: 3.28\n",
      "Epoch|it: 3|3500, Total Loss: 3.29\n",
      "Epoch|it: 3|4000, Total Loss: 3.28\n",
      "Epoch|it: 3|4500, Total Loss: 3.27\n",
      "Epoch|it: 3|5000, Total Loss: 3.28\n",
      "Epoch|it: 3|5500, Total Loss: 3.27\n",
      "Epoch|it: 3|6000, Total Loss: 3.27\n",
      "Epoch|it: 3|6500, Total Loss: 3.27\n",
      "Epoch|it: 3|7000, Total Loss: 3.27\n",
      "Epoch|it: 3|7500, Total Loss: 3.27\n",
      "Epoch|it: 3|8000, Total Loss: 3.27\n",
      "Epoch|it: 3|8500, Total Loss: 3.27\n",
      "Epoch 3 Loss: 3.27, Train/Test Accuracy: 0.645/0.647\n",
      "Epoch|it: 4|500, Total Loss: 3.27\n",
      "Epoch|it: 4|1000, Total Loss: 3.26\n",
      "Epoch|it: 4|1500, Total Loss: 3.24\n",
      "Epoch|it: 4|2000, Total Loss: 3.24\n",
      "Epoch|it: 4|2500, Total Loss: 3.23\n",
      "Epoch|it: 4|3000, Total Loss: 3.22\n",
      "Epoch|it: 4|3500, Total Loss: 3.22\n",
      "Epoch|it: 4|4000, Total Loss: 3.22\n",
      "Epoch|it: 4|4500, Total Loss: 3.21\n",
      "Epoch|it: 4|5000, Total Loss: 3.21\n",
      "Epoch|it: 4|5500, Total Loss: 3.21\n",
      "Epoch|it: 4|6000, Total Loss: 3.20\n",
      "Epoch|it: 4|6500, Total Loss: 3.21\n",
      "Epoch|it: 4|7000, Total Loss: 3.21\n",
      "Epoch|it: 4|7500, Total Loss: 3.20\n",
      "Epoch|it: 4|8000, Total Loss: 3.21\n",
      "Epoch|it: 4|8500, Total Loss: 3.20\n",
      "Epoch 4 Loss: 3.20, Train/Test Accuracy: 0.651/0.653\n",
      "Epoch|it: 5|500, Total Loss: 3.20\n",
      "Epoch|it: 5|1000, Total Loss: 3.20\n",
      "Epoch|it: 5|1500, Total Loss: 3.18\n",
      "Epoch|it: 5|2000, Total Loss: 3.18\n",
      "Epoch|it: 5|2500, Total Loss: 3.17\n",
      "Epoch|it: 5|3000, Total Loss: 3.16\n",
      "Epoch|it: 5|3500, Total Loss: 3.16\n",
      "Epoch|it: 5|4000, Total Loss: 3.16\n",
      "Epoch|it: 5|4500, Total Loss: 3.15\n",
      "Epoch|it: 5|5000, Total Loss: 3.15\n",
      "Epoch|it: 5|5500, Total Loss: 3.15\n",
      "Epoch|it: 5|6000, Total Loss: 3.14\n",
      "Epoch|it: 5|6500, Total Loss: 3.15\n",
      "Epoch|it: 5|7000, Total Loss: 3.15\n",
      "Epoch|it: 5|7500, Total Loss: 3.14\n",
      "Epoch|it: 5|8000, Total Loss: 3.15\n",
      "Epoch|it: 5|8500, Total Loss: 3.15\n",
      "Epoch 5 Loss: 3.14, Train/Test Accuracy: 0.657/0.658\n",
      "Epoch|it: 6|500, Total Loss: 3.15\n",
      "Epoch|it: 6|1000, Total Loss: 3.14\n",
      "Epoch|it: 6|1500, Total Loss: 3.12\n",
      "Epoch|it: 6|2000, Total Loss: 3.12\n",
      "Epoch|it: 6|2500, Total Loss: 3.11\n",
      "Epoch|it: 6|3000, Total Loss: 3.10\n",
      "Epoch|it: 6|3500, Total Loss: 3.11\n",
      "Epoch|it: 6|4000, Total Loss: 3.10\n",
      "Epoch|it: 6|4500, Total Loss: 3.09\n",
      "Epoch|it: 6|5000, Total Loss: 3.10\n",
      "Epoch|it: 6|5500, Total Loss: 3.09\n",
      "Epoch|it: 6|6000, Total Loss: 3.09\n",
      "Epoch|it: 6|6500, Total Loss: 3.09\n",
      "Epoch|it: 6|7000, Total Loss: 3.09\n",
      "Epoch|it: 6|7500, Total Loss: 3.09\n",
      "Epoch|it: 6|8000, Total Loss: 3.09\n",
      "Epoch|it: 6|8500, Total Loss: 3.09\n",
      "Epoch 6 Loss: 3.09, Train/Test Accuracy: 0.663/0.664\n",
      "Epoch|it: 7|500, Total Loss: 3.09\n",
      "Epoch|it: 7|1000, Total Loss: 3.09\n",
      "Epoch|it: 7|1500, Total Loss: 3.06\n",
      "Epoch|it: 7|2000, Total Loss: 3.07\n",
      "Epoch|it: 7|2500, Total Loss: 3.06\n",
      "Epoch|it: 7|3000, Total Loss: 3.05\n",
      "Epoch|it: 7|3500, Total Loss: 3.05\n",
      "Epoch|it: 7|4000, Total Loss: 3.05\n",
      "Epoch|it: 7|4500, Total Loss: 3.04\n",
      "Epoch|it: 7|5000, Total Loss: 3.04\n",
      "Epoch|it: 7|5500, Total Loss: 3.04\n",
      "Epoch|it: 7|6000, Total Loss: 3.03\n",
      "Epoch|it: 7|6500, Total Loss: 3.04\n",
      "Epoch|it: 7|7000, Total Loss: 3.04\n",
      "Epoch|it: 7|7500, Total Loss: 3.04\n",
      "Epoch|it: 7|8000, Total Loss: 3.04\n",
      "Epoch|it: 7|8500, Total Loss: 3.04\n",
      "Epoch 7 Loss: 3.04, Train/Test Accuracy: 0.669/0.669\n",
      "Epoch|it: 8|500, Total Loss: 3.04\n",
      "Epoch|it: 8|1000, Total Loss: 3.04\n",
      "Epoch|it: 8|1500, Total Loss: 3.01\n",
      "Epoch|it: 8|2000, Total Loss: 3.02\n",
      "Epoch|it: 8|2500, Total Loss: 3.01\n",
      "Epoch|it: 8|3000, Total Loss: 3.00\n",
      "Epoch|it: 8|3500, Total Loss: 3.00\n",
      "Epoch|it: 8|4000, Total Loss: 3.00\n",
      "Epoch|it: 8|4500, Total Loss: 2.99\n",
      "Epoch|it: 8|5000, Total Loss: 2.99\n",
      "Epoch|it: 8|5500, Total Loss: 2.99\n",
      "Epoch|it: 8|6000, Total Loss: 2.99\n",
      "Epoch|it: 8|6500, Total Loss: 2.99\n",
      "Epoch|it: 8|7000, Total Loss: 2.99\n",
      "Epoch|it: 8|7500, Total Loss: 2.99\n",
      "Epoch|it: 8|8000, Total Loss: 2.99\n",
      "Epoch|it: 8|8500, Total Loss: 2.99\n",
      "Epoch 8 Loss: 2.99, Train/Test Accuracy: 0.674/0.675\n",
      "Epoch|it: 9|500, Total Loss: 2.99\n",
      "Epoch|it: 9|1000, Total Loss: 2.99\n",
      "Epoch|it: 9|1500, Total Loss: 2.97\n",
      "Epoch|it: 9|2000, Total Loss: 2.97\n",
      "Epoch|it: 9|2500, Total Loss: 2.96\n",
      "Epoch|it: 9|3000, Total Loss: 2.95\n",
      "Epoch|it: 9|3500, Total Loss: 2.96\n",
      "Epoch|it: 9|4000, Total Loss: 2.95\n",
      "Epoch|it: 9|4500, Total Loss: 2.94\n",
      "Epoch|it: 9|5000, Total Loss: 2.95\n",
      "Epoch|it: 9|5500, Total Loss: 2.94\n",
      "Epoch|it: 9|6000, Total Loss: 2.94\n",
      "Epoch|it: 9|6500, Total Loss: 2.94\n",
      "Epoch|it: 9|7000, Total Loss: 2.94\n",
      "Epoch|it: 9|7500, Total Loss: 2.94\n",
      "Epoch|it: 9|8000, Total Loss: 2.94\n",
      "Epoch|it: 9|8500, Total Loss: 2.94\n",
      "Epoch 9 Loss: 2.94, Train/Test Accuracy: 0.68/0.681\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "s2s.train()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    s2s.train()\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "    y_train_pred = []\n",
    "    y_train_true = []\n",
    "    for it, example in enumerate(train_data):\n",
    "        if (it % 500 == 0) and (it != 0):\n",
    "            print(\"Epoch|it: {}|{}, Total Loss: {:.2f}\".format(epoch, it, total_loss / it))\n",
    "        input_seq, output_seq = example\n",
    "        enc_optim.zero_grad()\n",
    "        dec_optim.zero_grad()\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)    \n",
    "        res = s2s.forward(input_seq, output_seq)\n",
    "        loss = criterion(res, torch.LongTensor(output_seq))\n",
    "        loss.backward()\n",
    "        total_loss += loss.data.numpy()\n",
    "\n",
    "        enc_optim.step()\n",
    "        dec_optim.step()\n",
    "        \n",
    "        preds = list(torch.argmax(res, dim=1).data.numpy())\n",
    "        trues = list(output_seq.data.numpy())\n",
    "        y_train_true.extend(trues)\n",
    "        y_train_pred.extend(preds)\n",
    "        \n",
    "    a_train = accuracy_score(y_train_true, y_train_pred)\n",
    "    \n",
    "    s2s.eval()\n",
    "    y_test_pred = []\n",
    "    y_test_true = []\n",
    "\n",
    "    for example in test_data:\n",
    "\n",
    "        input_seq, output_seq = example\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)    \n",
    "        res = s2s.forward(input_seq, output_seq)\n",
    "        preds = list(torch.argmax(res, dim=1).data.numpy())\n",
    "        trues = list(output_seq.data.numpy())\n",
    "\n",
    "        y_test_true.extend(trues)\n",
    "        y_test_pred.extend(preds)\n",
    "\n",
    "    a_test = accuracy_score(y_test_true, y_test_pred)\n",
    "    \n",
    "    print(\"Epoch {} Loss: {:.2f}, Train/Test Accuracy: {:.3}/{:.3f}\".format(epoch, total_loss / it, a_train, a_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/serialization.py:250: UserWarning: Couldn't retrieve source code for container of type seq2seq. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.seq2seq'>: it's not the same object as __main__.seq2seq",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-333-1c2fa72356c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq2seq\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../data/seq2seq.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0mpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0mserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized_storages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class '__main__.seq2seq'>: it's not the same object as __main__.seq2seq"
     ]
    }
   ],
   "source": [
    "from modules.seq2seq import encoder, decoder, seq2seq\n",
    "torch.save(s2s, '../data/seq2seq.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
