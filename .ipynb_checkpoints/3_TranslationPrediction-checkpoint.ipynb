{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.nn import Linear, Embedding, RNN, GRU, LSTM\n",
    "from torch.nn import Sigmoid, LogSoftmax\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss, NLLLoss, CrossEntropyLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax()\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)\n",
    "        e = e.view(len(x), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        return out, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
    "        super(decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax()\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, input, hidden):\n",
    "        self.hidden = hidden\n",
    "        e = self.embedding(input)\n",
    "        e = e.view(len(input), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        output = self.linear(out[0])\n",
    "        so = self.softmax(output)\n",
    "        return so, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class seq2seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "                \n",
    "    def forward(self, input_seq, output_seq, p_tf=0):\n",
    "        outputs = []\n",
    "        \n",
    "        enc.hidden = self.enc.init_hidden()\n",
    "        dec.hidden = self.dec.init_hidden()        \n",
    "        \n",
    "        enc_output, enc_hidden = enc(torch.LongTensor(input_seq))\n",
    "        context = enc_output[-1].unsqueeze(0)\n",
    "        \n",
    "        dec_output, hidden = dec(torch.LongTensor([output_seq[0]]), (context, context))\n",
    "        outputs.append(dec_output)\n",
    "        for i in range(1,output_seq.shape[0]):\n",
    "            dec_input = torch.LongTensor([output_seq[i]])\n",
    "            dec_output, hidden = dec(dec_input, hidden) \n",
    "            outputs.append(dec_output)\n",
    "        return torch.stack(outputs).squeeze(1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_indices = pickle.load(open('../data/translation_indices.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for seq2seq:\n\tsize mismatch for enc.embedding.weight: copying a param with shape torch.Size([17267, 100]) from checkpoint, the shape in current model is torch.Size([17148, 100]).\n\tsize mismatch for dec.embedding.weight: copying a param with shape torch.Size([21854, 50]) from checkpoint, the shape in current model is torch.Size([21842, 50]).\n\tsize mismatch for dec.linear.weight: copying a param with shape torch.Size([21854, 50]) from checkpoint, the shape in current model is torch.Size([21842, 50]).\n\tsize mismatch for dec.linear.bias: copying a param with shape torch.Size([21854]) from checkpoint, the shape in current model is torch.Size([21842]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-0be675ba3d71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/s2s.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0ms2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 769\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_named_members\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for seq2seq:\n\tsize mismatch for enc.embedding.weight: copying a param with shape torch.Size([17267, 100]) from checkpoint, the shape in current model is torch.Size([17148, 100]).\n\tsize mismatch for dec.embedding.weight: copying a param with shape torch.Size([21854, 50]) from checkpoint, the shape in current model is torch.Size([21842, 50]).\n\tsize mismatch for dec.linear.weight: copying a param with shape torch.Size([21854, 50]) from checkpoint, the shape in current model is torch.Size([21842, 50]).\n\tsize mismatch for dec.linear.bias: copying a param with shape torch.Size([21854]) from checkpoint, the shape in current model is torch.Size([21842])."
     ]
    }
   ],
   "source": [
    "input_size = len(translation_indices['input2idx'])\n",
    "output_size = len(translation_indices['output2idx'])\n",
    "\n",
    "enc_vocab_size = input_size\n",
    "enc_embedding_dim = 100\n",
    "enc_hidden_dim = 50\n",
    "\n",
    "dec_vocab_size = output_size\n",
    "dec_embedding_dim = 50\n",
    "dec_hidden_dim = 50\n",
    "dec_output_dim = output_size\n",
    "\n",
    "enc = encoder(enc_vocab_size, enc_embedding_dim, enc_hidden_dim, batch_size=1)\n",
    "dec = decoder(dec_vocab_size, dec_embedding_dim, dec_hidden_dim, dec_output_dim, batch_size=1)\n",
    "s2s = seq2seq(enc, dec)\n",
    "\n",
    "state_dict = torch.load('../data/s2s.pt')\n",
    "s2s.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('../data/s2s.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17148"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21842"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
