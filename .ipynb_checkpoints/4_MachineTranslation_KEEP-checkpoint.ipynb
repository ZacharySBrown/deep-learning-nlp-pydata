{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.nn import Linear, Embedding, RNN, GRU, LSTM\n",
    "from torch.nn import Sigmoid, LogSoftmax\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import BCELoss, NLLLoss, CrossEntropyLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11618, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>spanish</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600702</th>\n",
       "      <td>Allow us to state that the opening of the inqu...</td>\n",
       "      <td>Permita que le digamos que las investigaciones...</td>\n",
       "      <td>[&lt;SOS&gt;, allow, us, to, state, that, the, openi...</td>\n",
       "      <td>[&lt;SOS&gt;, permita, que, le, digamos, que, las, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>Since June of last year, the OLAF regulation h...</td>\n",
       "      <td>Desde junio del año pasado el Reglamento de la...</td>\n",
       "      <td>[&lt;SOS&gt;, since, june, of, last, year, the, olaf...</td>\n",
       "      <td>[&lt;SOS&gt;, desde, junio, del, año, pasado, el, re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180999</th>\n",
       "      <td>I am fully aware of the views expressed here o...</td>\n",
       "      <td>Soy plenamente consciente de las opiniones exp...</td>\n",
       "      <td>[&lt;SOS&gt;, i, am, fully, aware, of, the, views, e...</td>\n",
       "      <td>[&lt;SOS&gt;, soy, plenamente, consciente, de, las, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109251</th>\n",
       "      <td>I will have clarification of that situation la...</td>\n",
       "      <td>Hoy mismo recibiré información sobre dicha sit...</td>\n",
       "      <td>[&lt;SOS&gt;, i, will, have, clarification, of, that...</td>\n",
       "      <td>[&lt;SOS&gt;, hoy, mismo, recibiré, información, sob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401751</th>\n",
       "      <td>Let us not forget, the Roma were the first to ...</td>\n",
       "      <td>No olvidemos que la población gitana fue la pr...</td>\n",
       "      <td>[&lt;SOS&gt;, let, us, not, forget, the, roma, were,...</td>\n",
       "      <td>[&lt;SOS&gt;, no, olvidemos, que, la, población, git...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   english  \\\n",
       "600702   Allow us to state that the opening of the inqu...   \n",
       "3733     Since June of last year, the OLAF regulation h...   \n",
       "1180999  I am fully aware of the views expressed here o...   \n",
       "109251   I will have clarification of that situation la...   \n",
       "1401751  Let us not forget, the Roma were the first to ...   \n",
       "\n",
       "                                                   spanish  \\\n",
       "600702   Permita que le digamos que las investigaciones...   \n",
       "3733     Desde junio del año pasado el Reglamento de la...   \n",
       "1180999  Soy plenamente consciente de las opiniones exp...   \n",
       "109251   Hoy mismo recibiré información sobre dicha sit...   \n",
       "1401751  No olvidemos que la población gitana fue la pr...   \n",
       "\n",
       "                                                      text  \\\n",
       "600702   [<SOS>, allow, us, to, state, that, the, openi...   \n",
       "3733     [<SOS>, since, june, of, last, year, the, olaf...   \n",
       "1180999  [<SOS>, i, am, fully, aware, of, the, views, e...   \n",
       "109251   [<SOS>, i, will, have, clarification, of, that...   \n",
       "1401751  [<SOS>, let, us, not, forget, the, roma, were,...   \n",
       "\n",
       "                                                     label  \n",
       "600702   [<SOS>, permita, que, le, digamos, que, las, i...  \n",
       "3733     [<SOS>, desde, junio, del, año, pasado, el, re...  \n",
       "1180999  [<SOS>, soy, plenamente, consciente, de, las, ...  \n",
       "109251   [<SOS>, hoy, mismo, recibiré, información, sob...  \n",
       "1401751  [<SOS>, no, olvidemos, que, la, población, git...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle('../data/4_europarl_en_sp.pkl')\n",
    "data['text'] = data['english'].map(lambda x: \"\".join([i for i in x.lower() if i not in string.punctuation]).split())\n",
    "data['label'] = data['spanish'].map(lambda x: \"\".join([i for i in x.lower() if i not in string.punctuation]).split())\n",
    "\n",
    "data['text'] = data['text'].map(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "data['label'] = data['label'].map(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "print(data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_words = set(itertools.chain.from_iterable(data['text']))\n",
    "output_words = set(itertools.chain.from_iterable(data['label']))\n",
    "\n",
    "input2idx = {word: idx for idx, word in enumerate(input_words)}\n",
    "idx2input = {idx: word for word, idx in input2idx.items()}\n",
    "\n",
    "output2idx = {word: idx for idx, word in enumerate(output_words)}\n",
    "idx2output = {idx: word for word, idx in output2idx.items()}\n",
    "\n",
    "input_size = len(input_words)\n",
    "output_size = len(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seqs = data['text'].map(lambda x: [input2idx[i] for i in x]).tolist()\n",
    "output_seqs = data['label'].map(lambda x: [output2idx[i] for i in x]).tolist()\n",
    "\n",
    "data = list(zip(input_seqs, output_seqs))\n",
    "\n",
    "train_data, test_data = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax()\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)\n",
    "        e = e.view(len(x), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        return out, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
    "        super(decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax(dim=1)\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, input, hidden):\n",
    "        self.hidden = hidden\n",
    "        e = self.embedding(input)\n",
    "        e = e.view(len(input), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        self.out = out\n",
    "        output = self.linear(out[0])\n",
    "        so = self.softmax(output)\n",
    "        return so, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class seq2seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "                \n",
    "    def forward(self, input_seq, output_seq, p_tf=0):\n",
    "        outputs = []\n",
    "        \n",
    "        self.enc.hidden = self.enc.init_hidden()\n",
    "        self.dec.hidden = self.dec.init_hidden()        \n",
    "        \n",
    "        enc_output, enc_hidden = enc.forward(torch.LongTensor(input_seq))\n",
    "        dec_hidden = enc_hidden\n",
    "        tf_cnt = 0\n",
    "        for i in range(output_seq.shape[0]):  \n",
    "            \n",
    "            if (np.random.uniform()) > p_tf and (i != 0):\n",
    "                dec_input = torch.LongTensor([torch.argmax(dec_output).data])\n",
    "            else:\n",
    "                dec_input = torch.LongTensor([output_seq[i]])\n",
    "                \n",
    "            dec_output, dec_hidden = self.dec.forward(dec_input, dec_hidden) \n",
    "            outputs.append(dec_output)\n",
    "            \n",
    "        return torch.stack(outputs).squeeze(1)\n",
    "    \n",
    "    def predict(self, input_seq, sos_idx, eos_idx, max_len=20):\n",
    "        outputs = []\n",
    "        self.enc.hidden = self.enc.init_hidden()\n",
    "        self.dec.hidden = self.dec.init_hidden()   \n",
    "        \n",
    "        enc_output, enc_hidden = enc.forward(torch.LongTensor(input_seq))\n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        cnt = 0\n",
    "        dec_input = torch.LongTensor([sos_idx])\n",
    "        \n",
    "        dec_output, dec_hidden = self.dec.forward(dec_input, dec_hidden) \n",
    "        \n",
    "        output_idx = torch.argmax(dec_output).data\n",
    "        \n",
    "        while (int(output_idx) != eos_idx) and (cnt <= max_len): \n",
    "            cnt += 1\n",
    "            dec_input = torch.LongTensor([output_idx])        \n",
    "            dec_output, dec_hidden = self.dec.forward(dec_input, dec_hidden) \n",
    "            output_idx = torch.argmax(dec_output).data\n",
    "            outputs.append(int(output_idx))\n",
    "            \n",
    "            \n",
    "        return outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = input_size\n",
    "enc_embedding_dim = 100\n",
    "enc_hidden_dim = 100\n",
    "\n",
    "dec_vocab_size = output_size\n",
    "dec_embedding_dim = 100\n",
    "dec_hidden_dim = 100\n",
    "dec_output_dim = output_size\n",
    "\n",
    "enc = encoder(enc_vocab_size, enc_embedding_dim, enc_hidden_dim, batch_size=1)\n",
    "dec = decoder(dec_vocab_size, dec_embedding_dim, dec_hidden_dim, dec_output_dim, batch_size=1)\n",
    "s2s = seq2seq(enc, dec)\n",
    "\n",
    "\n",
    "optim = SGD(params=s2s.parameters(), lr=0.01)\n",
    "criterion = NLLLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch|it: 0|100, Total Loss: 9.64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c3fc42e6dc43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep-learning-nlp/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    s2s.train()\n",
    "    total_loss = 0\n",
    "    s2s.train()\n",
    "    for it, example in enumerate(train_data):\n",
    "\n",
    "        if (it % 100 == 0) and (it != 0):\n",
    "            print(\"Epoch|it: {}|{}, Total Loss: {:.2f}\".format(epoch, it, total_loss / it))\n",
    "        input_seq, output_seq = example\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)\n",
    "\n",
    "        res = s2s.forward(input_seq, output_seq[:-1], p_tf=0.5)\n",
    "        loss = criterion(res, output_seq[1:])\n",
    "        loss.backward()\n",
    "        total_loss += loss.data.numpy()\n",
    "\n",
    "        optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['subsanar',\n",
       " 'freno',\n",
       " 'cornelissen',\n",
       " 'electricidad',\n",
       " 'retira',\n",
       " 'adherirme',\n",
       " 'inauguración',\n",
       " 'cerraron',\n",
       " 'su',\n",
       " 'recibimos',\n",
       " 'obvio',\n",
       " 'obvio',\n",
       " 'coincidencia',\n",
       " 'continua',\n",
       " '¿las',\n",
       " 'le',\n",
       " 'competencia',\n",
       " 'degradación',\n",
       " 'precisa',\n",
       " 'martín',\n",
       " 'supuso']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_idx = output2idx['<SOS>']\n",
    "eos_idx = output2idx['<EOS>']\n",
    "pred_idxs = s2s.predict(input_seq, sos_idx, eos_idx)\n",
    "[idx2output[i] for i in pred_idxs]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
