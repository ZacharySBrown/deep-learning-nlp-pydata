{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import getpass\n",
    "\n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = \"/Users/{}/anaconda3/envs/rise_latest/etc/jupyter/nbconfig\".format(getpass.getuser())\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "o = cm.update(\"livereveal\", {\n",
    "              \"theme\": \"sky\",\n",
    "              \"transition\": \"fade\",\n",
    "              \"start_slideshow_at\": \"selected\",\n",
    "})\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h1> Deep Learning and Modern Natural Language Processing </h1>\n",
    "<br>\n",
    "Zachary S. Brown\n",
    "<br>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Outline\n",
    "0. NLP Problem Structure\n",
    "1. Text Classification and the Perceptron\n",
    "2. Vectorization and Classification with RNNs\n",
    "3. POS Tagging with RNNs\n",
    "4. Sequence to Sequence Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP Problem Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## General Problem Structure\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_0.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary Document Classification\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_1.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-class Document Classification\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_2.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-class _Sequence_ Classification\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_4.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Starting Easy: Neural Net with Traditional Vectorization\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_5.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Text Classification and the Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics\n",
    "* The perceptron and neural network optimization\n",
    "* Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Perceptron\n",
    "<center>\n",
    "<img src=\"src/0_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Weights\n",
    "<center>\n",
    "<img src=\"src/1_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forward Pass\n",
    "<center>\n",
    "<img src=\"src/2_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Loss\n",
    "<center>\n",
    "<img src=\"src/3_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Calculate Gradients\n",
    "<center>\n",
    "<img src=\"src/4_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Update Weights\n",
    "<center>\n",
    "<img src=\"src/5_Perceptron.png?\" alt=\"perceptron\" style=\"height:400px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Perceptron Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.nn import Linear\n",
    "from torch.nn import Sigmoid, LogSoftmax\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss, NLLLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## 0. Dataset Loading and Cleaning\n",
    "\n",
    "We'll begin by loading a prepared version of the [Stanford Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/), on which we'll train a binary classifier. \n",
    "\n",
    "This dataset contains 50k highly polarized movie reviews from IMDB, labeled with positive or negative sentiment. \n",
    "\n",
    "We'll perform some minimal preprocessing on the text itself, simply case-normalization and removal of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data into a DataFrame\n",
    "data = pd.read_pickle('../data/1a_acl_imdb.pkl')\n",
    "\n",
    "# Definne a simple convenience function for cleaning the strings\n",
    "def clean_text(text):\n",
    "    return \"\".join([c for c in text.lower() if c not in punctuation])\n",
    "\n",
    "# Clean the string labels\n",
    "data['text_cleaned'] = data['text'].map(clean_text)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 1. Text Vectorization\n",
    "\n",
    "Once we have some (somewhat) clean data, we can then vectorize the corpus the standard term frequency, inverse document frequency. For the sake of time, we'll limit the overall input feature space to the top 1k tokens, based on the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize a TfidfVectorizer Object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit the cleaned text\n",
    "tfidf.fit(data['text_cleaned'])\n",
    "\n",
    "# Examine the total number of tokens in the text\n",
    "print(\"Total tokens in input corpus: {}\".format(len(tfidf.vocabulary_)))\n",
    "\n",
    "# Initialize a TfidfVectorizer Object, this time with a max number of features\n",
    "max_features = 1000\n",
    "tfidf = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "# Fit the cleaned text\n",
    "features = tfidf.fit_transform(data['text_cleaned']).todense()\n",
    "labels = data.label.values.reshape(-1,1)\n",
    "\n",
    "# Create tuples of the feature/label pairs, \n",
    "# and perform a stratified train/test split\n",
    "all_data = list(zip(features, labels))\n",
    "train_data, test_data = train_test_split(all_data, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2a. Perceptron Classifier\n",
    "\n",
    "For the simplest perceptron, we'll only need a single linear layer as well as a sigmoid transformation to map the output space from our linear layer into the proper probability distribution. \n",
    "\n",
    "Two other things that need to be considered are the choice of loss funciton and the optimization algorithm. We'll use binary cross entropy for the loss function, and stochastic gradient descent for the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Create a linear single linear layer, \n",
    "# with input shape of our feature space \n",
    "# and output shape of 1 (binary classification)\n",
    "linear = Linear(max_features, 1, bias=True)\n",
    "\n",
    "# Create a instance of the sigmoid function\n",
    "# so we can normalize our output to the range [0,1]\n",
    "sigmoid = Sigmoid()\n",
    "\n",
    "# Binary cross entropy is an appropriate loss function \n",
    "# for this type of problem, and is implemented in the \n",
    "# `BCELoss` class in pytroch\n",
    "criterion = BCELoss()\n",
    "\n",
    "# We'll use basic stochastic gradient descent\n",
    "# to optimize the parameters of our linear layer \n",
    "# (the sigmoid is a transformation with no parameters)\n",
    "optim = SGD(params=linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# let's see what it looks like \n",
    "# to pass a single example through\n",
    "# the objects above\n",
    "\n",
    "# grab a single example\n",
    "f = features[0]\n",
    "t = labels[0]\n",
    "\n",
    "# conver the example to tensors\n",
    "X = torch.FloatTensor(f)\n",
    "y = torch.FloatTensor(t)\n",
    "print(\"Shape of feature tensor:\", X.shape)\n",
    "\n",
    "# pass the input tensor through the linear layer\n",
    "linear_output = linear.forward(X)\n",
    "print(\"Shape of linear_output:\", linear_output.shape)\n",
    "\n",
    "# take the sigmoid of the linear output\n",
    "sigmoid_output = sigmoid(linear_output)\n",
    "print(\"Value of sigmoid_output:\", sigmoid_output)\n",
    "\n",
    "# calculat the loss w.r.t. the expected value\n",
    "loss = criterion(sigmoid_output.view(1,-1), y)\n",
    "print(\"Value of loss:\", loss)\n",
    "\n",
    "# calculate the gradients\n",
    "loss.backward()\n",
    "\n",
    "# check the current value and gradient\n",
    "# of the bias\n",
    "weights, bias = list(linear.parameters())\n",
    "print(\"Bias:\", bias.data)\n",
    "print(\"Bias gradient:\", bias.grad)\n",
    "\n",
    "# take a step with the optimizer\n",
    "# to update the parameters\n",
    "optim.step()\n",
    "\n",
    "# check the value of the bias\n",
    "weights, bias = list(linear.parameters())\n",
    "print(\"Bias:\", bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2b. Training the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "for it, example in tqdm(list(enumerate(train_data))):\n",
    "    \n",
    "    # zero out our gradients for each weight\n",
    "    optim.zero_grad()\n",
    "    \n",
    "    # pull out the features and the target\n",
    "    # for each example\n",
    "    f, t = example\n",
    "    \n",
    "    # cast the feature and target to \n",
    "    # the appropriate torch types\n",
    "    X = torch.FloatTensor(f)\n",
    "    y = torch.FloatTensor(t)\n",
    "    \n",
    "    # start the forward pass\n",
    "    X_prime = linear(X)\n",
    "    output = sigmoid(X_prime)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(output.view(-1), y)\n",
    "    total_loss += loss.data.numpy()\n",
    "    \n",
    "    # calcualte the gradients of the\n",
    "    # loss w.r.t. each of the parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update the weights in the\n",
    "    # linear layer\n",
    "    optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2c. Evaluating the Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# initialize lists to keep track\n",
    "# of predicted and actual values\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# set a probability threshold \n",
    "# for calculating the accuracy\n",
    "threshold = 0.5\n",
    "\n",
    "# loop through the test examples\n",
    "for f, t in test_data:\n",
    "    X = torch.FloatTensor(f)\n",
    "    y = torch.FloatTensor(t)\n",
    "    output = sigmoid(linear(X))\n",
    "    y_true.append(y.data.numpy()[0])\n",
    "    y_pred.append(output.data.numpy()[0])\n",
    "\n",
    "# calculate a prediction,\n",
    "# then compute the accuracy\n",
    "y_pred = [int(p >= threshold) for p in y_pred]\n",
    "a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "print(\"Validation Accuracy: {:.2f}\".format(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2e. Creating a Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from modules.perceptron import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 2d. Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from modules.perceptron import *\n",
    "\n",
    "\n",
    "model = perceptron(max_features)\n",
    "criterion = BCELoss()\n",
    "optim = SGD(params=model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for it, example in enumerate(train_data):\n",
    "            optim.zero_grad()\n",
    "            f, t = example\n",
    "            X = torch.FloatTensor(f)\n",
    "            y = torch.FloatTensor(t)\n",
    "            output = model.forward(X)\n",
    "            loss = criterion(output.view(-1), y)\n",
    "            total_loss += loss.data.numpy()\n",
    "            loss.backward()\n",
    "\n",
    "            optim.step()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        threshold = 0.5\n",
    "        for f, t in test_data:\n",
    "            X = torch.FloatTensor(f)\n",
    "            y = torch.FloatTensor(t)\n",
    "            output = model.forward(X)\n",
    "            y_true.append(y.data.numpy()[0])\n",
    "            y_pred.append(output.data.numpy()[0])\n",
    "\n",
    "        y_pred = [int(p >= threshold) for p in y_pred]\n",
    "        a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "        total_loss /= (it + 1)\n",
    "        print(\"Epoch Loss: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# 3. Multi-class Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load the data into a DataFrame\n",
    "data = pd.read_pickle('../data/1b_stackoverflow_qna.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Definne a simple convenience function for cleaning the strings\n",
    "def clean_text(text):\n",
    "    return \"\".join([c for c in text.lower() if c not in punctuation])\n",
    "\n",
    "# Clean the string labels\n",
    "data['text_cleaned'] = data['text'].map(clean_text)\n",
    "\n",
    "# Initialize a TfidfVectorizer Object\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit the cleaned text\n",
    "tfidf.fit(data['text_cleaned'])\n",
    "\n",
    "# Examine the total number of tokens in the text\n",
    "print(\"Total tokens in input corpus: {}\".format(len(tfidf.vocabulary_)))\n",
    "\n",
    "# Initialize a TfidfVectorizer Object, this time with a max number of features\n",
    "max_features = 1000\n",
    "tfidf = TfidfVectorizer(max_features=max_features)\n",
    "\n",
    "# Fit the cleaned text\n",
    "features = tfidf.fit_transform(data['text_cleaned']).todense()\n",
    "\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data.label.values).reshape(-1,1)\n",
    "label_size = len(le.classes_)\n",
    "\n",
    "# Create tuples of the feature/label pairs, \n",
    "# and perform a stratified train/test split\n",
    "all_data = list(zip(features, labels))\n",
    "train_data, test_data = train_test_split(all_data, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Create a linear single linear layer, \n",
    "# with input shape of our feature space \n",
    "# and output shape of 1 (binary classification)\n",
    "linear = Linear(max_features, label_size, bias=True)\n",
    "\n",
    "# Create a instance of the sigmoid function\n",
    "# so we can normalize our output to the range [0,1]\n",
    "softmax = LogSoftmax(dim=1)\n",
    "\n",
    "# Binary cross entropy is an appropriate loss function \n",
    "# for this type of problem, and is implemented in the \n",
    "# `BCELoss` class in pytorch\n",
    "criterion = NLLLoss()\n",
    "\n",
    "# We'll use basic stochastic gradient descent\n",
    "# to optimize the parameters of our linear layer \n",
    "# (the sigmoid is a transformation with no parameters)\n",
    "optim = SGD(params=linear.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# take an example from features and labels\n",
    "f = features[0]\n",
    "t = labels[0]\n",
    "\n",
    "# cast the features and targets\n",
    "# as torch tensors\n",
    "X = torch.FloatTensor(f)\n",
    "y = torch.LongTensor(t)\n",
    "print(\"Features Tensor Shape:\", X.shape)\n",
    "print(\"Features Tensor Shape:\", y.shape)\n",
    "\n",
    "# pass the features tensor through the linear layer\n",
    "linear_output = linear(X)\n",
    "print(\"Linear Output Shape:\", linear_output.shape)\n",
    "\n",
    "# pass the output from the linear\n",
    "# layer through the softmax\n",
    "softmax_output = softmax(linear_output)\n",
    "print(\"Softmax Output Shape:\", softmax_output.shape)\n",
    "\n",
    "# verify that the softmax actually sums to 1\n",
    "# HINT: use torch.exp (this is LogSoftmax) and torch.sum\n",
    "softmax_normalization = torch.exp(softmax_output).sum()\n",
    "print(\"Softmax Normalization:\", softmax_normalization)\n",
    "\n",
    "# calculate the loss\n",
    "loss = criterion(softmax_output, y)\n",
    "print(\"Loss:\", loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model = multi_class_perceptron(max_features, label_size)\n",
    "optim = SGD(params=model.parameters(), lr=0.01)\n",
    "criterion = NLLLoss()\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    linear.train()\n",
    "    total_loss = 0\n",
    "    for it, example in list(enumerate(train_data)):\n",
    "        optim.zero_grad()\n",
    "        f, t = example\n",
    "        X = torch.FloatTensor(f)\n",
    "        y = torch.LongTensor(t)\n",
    "        output = model.forward(X)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.data.numpy()\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "        \n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    threshold = 0.5\n",
    "\n",
    "    for f, t in test_data:\n",
    "        X = torch.FloatTensor(f)\n",
    "        y = torch.FloatTensor([t])\n",
    "        output = model.forward(X)\n",
    "        y_true.append(y.data.numpy()[0])\n",
    "        y_pred.append(torch.argmax(output.data).numpy())\n",
    "\n",
    "\n",
    "    a = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    total_loss /= (it + 1)\n",
    "\n",
    "    print(\"Loss: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
