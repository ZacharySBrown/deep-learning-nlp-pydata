{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import getpass\n",
    "\n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = \"/Users/{}/anaconda3/envs/rise_latest/etc/jupyter/nbconfig\".format(getpass.getuser())\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "o = cm.update(\"livereveal\", {\n",
    "              \"theme\": \"sky\",\n",
    "              \"transition\": \"fade\",\n",
    "              \"start_slideshow_at\": \"selected\",\n",
    "})\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence to Sequence Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topics\n",
    "* Encoder-Decoder Architecture\n",
    "* Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Variable Length Sequence to Sequence\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_8.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-to-Sequence Overview\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/0_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Encoder-Decoder Architecture\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/1_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-to-Sequence Training: Input\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/2_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-to-Sequence Training: Loss\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/3_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-to-Sequence Training: Teacher Forcing\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/3a_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence-to-Sequence Inference\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/4_encoder_decoder.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.nn import Linear, Embedding, RNN, GRU, LSTM\n",
    "from torch.nn import Sigmoid, LogSoftmax\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import BCELoss, NLLLoss, CrossEntropyLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../data/4_europarl_en_sp.pkl')\n",
    "data['text'] = data['english'].map(lambda x: \"\".join([i for i in x.lower() if i not in string.punctuation]).split())\n",
    "data['label'] = data['spanish'].map(lambda x: \"\".join([i for i in x.lower() if i not in string.punctuation]).split())\n",
    "\n",
    "data['text'] = data['text'].map(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "data['label'] = data['label'].map(lambda x: ['<SOS>'] + x + ['<EOS>'])\n",
    "print(data.shape)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_words = set(itertools.chain.from_iterable(data['text']))\n",
    "output_words = set(itertools.chain.from_iterable(data['label']))\n",
    "\n",
    "input2idx = {word: idx for idx, word in enumerate(input_words)}\n",
    "idx2input = {idx: word for word, idx in input2idx.items()}\n",
    "\n",
    "output2idx = {word: idx for idx, word in enumerate(output_words)}\n",
    "idx2output = {idx: word for word, idx in output2idx.items()}\n",
    "\n",
    "input_size = len(input_words)\n",
    "output_size = len(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_seqs = data['text'].map(lambda x: [input2idx[i] for i in x]).tolist()\n",
    "output_seqs = data['label'].map(lambda x: [output2idx[i] for i in x]).tolist()\n",
    "\n",
    "data = list(zip(input_seqs, output_seqs))\n",
    "\n",
    "train_data, test_data = train_test_split(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we'll want to create an embedding layer for our encoder\n",
    "# TODO: create an encoder embedding layer\n",
    "\n",
    "# Next we'll want to create an LSTM for our encoder\n",
    "# TODO: create an LSTM for the encoder\n",
    "\n",
    "# Next we'll want to create an embedding layer for our decoder\n",
    "# TODO: create an embedding layer for the decoder\n",
    "\n",
    "# Next, create an LSTM for the decoder\n",
    "# TODO: create an LSTM for the decoder\n",
    "\n",
    "# When we read the output from the decoder network we'll\n",
    "# want to classify it as one of the words from the output corpus\n",
    "# to do this, we'll need a linear layer\n",
    "# TODO: create an linear layer for the decoder LSTM output\n",
    "\n",
    "# Lastly, we'll need an instance of LogSoftmax to convert the\n",
    "# output to a softmax distribution for feeding into NLLLoss\n",
    "# Hint: Set dim=1 on initialization here\n",
    "# TODO: create an instance of LogSoftmax\n",
    "\n",
    "# Create an instance of the NLLLoss\n",
    "# TODO: create an instance of the NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab an example input and output sequence:\n",
    "input_seq = input_seqs[0]\n",
    "output_seq = output_seqs[0]\n",
    "\n",
    "# convert these to torch tensors\n",
    "input_seq_tensor = #TODO\n",
    "output_seq_tensor = #TODO\n",
    "print(\"Input Sequence Tensor Shape:\", input_seq_tensor.shape)\n",
    "print(\"Output Sequence Tensor Shape:\", output_seq_tensor.shape)\n",
    "\n",
    "# pass the input sequence through the encoder embedding\n",
    "enc_embedded = #TODO\n",
    "print(\"Encoder Embedded Sequence Shape:\", enc_embedded.shape)\n",
    "\n",
    "# unsqueeze the embedding tensor to have a batch size of 1\n",
    "enc_embedded_unsqueezed = #TODO\n",
    "print(\"Encoder Embedded Sequence Shape (1 batch):\", enc_embedded.shape)\n",
    "\n",
    "# create initial hidden states for the encoder LSTM\n",
    "# e.g. example for hidden dim of 50:\n",
    "h0 = torch.zeros(1, 1, 50)\n",
    "c0 = torch.zeros(1, 1, 50)\n",
    "enc_hidden = (h0, c0)\n",
    "    \n",
    "\n",
    "# pass the embedded input sequence through the encoder LSTM\n",
    "enc_out, enc_hidden = #\n",
    "print(\"Encoder Output Shape:\", enc_out.shape)\n",
    "print(\"Encoder Hidden Shape(s):\", enc_hidden[0].shape, enc_hidden[1].shape)\n",
    "\n",
    "# set the decoder rnn initial hidden state \n",
    "# to the last hidden state of the encoder rnn\n",
    "dec_hidden = enc_hidden\n",
    "\n",
    "# pass the output sequence through the decoder embedding layer\n",
    "dec_embedded = #TODO\n",
    "print(\"Decoder Embedded Sequence Shape:\", dec_embedded.shape)\n",
    "\n",
    "# unsqueeze the embedding tensor to have a batch size of 1\n",
    "dec_embedded_unsqueezed = #TODO\n",
    "print(\"Decoder Embedded Sequence Shape (1 batch):\", dec_embedded.shape)\n",
    "\n",
    "# Assuming only teacher forcing (running true answer as input to the decoder)\n",
    "# Run the output sequence through the dec LSTM\n",
    "# Note: we want to pass ALL BUT THE LAST elemebt of the output \n",
    "# sequence through the LSTM\n",
    "\n",
    "# Create the input for the lstm in (all but the last element)\n",
    "dec_lstm_in = #TODO\n",
    "print(\"Decoder LSTM Input Shape:\", dec_lstm_in.shape)\n",
    "\n",
    "# Pass the decoder input through the decoder\n",
    "dec_out, dec_hidden = #TODO\n",
    "print(\"Decoder Output Shape:\", dec_out.shape)\n",
    "print(\"Decoder Hidden Shape(s):\", dec_hidden[0].shape, dec_hidden[1].shape)\n",
    "\n",
    "# Now we want to run the decoder output through our decoder linear layer\n",
    "# Also, squeeze the batch dimension (dim=1) of the linear output to get rid of it\n",
    "dec_linear_output = #TODO\n",
    "print(\"Decoder Linear Output Shape:\", dec_linear_output.shape)\n",
    "\n",
    "# pass the decoder linear output through a softmax\n",
    "dec_softmax_output = #TODO\n",
    "print(\"Decoder Softmax Output Shape:\", dec_softmax_output.shape)\n",
    "\n",
    "# verify that the decoder output distributions is _actually_ a softmax\n",
    "# Hint: use torch.exp and torch.sum\n",
    "dec_softmax_norms = #TODO\n",
    "print(\"Decoder Softmax Norms Shape:\", dec_softmax_norms.shape)\n",
    "print(\"Decoder Softmax Norms:\", dec_softmax_norms)\n",
    "\n",
    "# the targets for the loss function should be\n",
    "# ALL BUT THE FIRST element of the output sequence\n",
    "dec_loss_target = #TODO\n",
    "print(\"Decoder Loss Target Shape:\", dec_loss_target.shape)\n",
    "\n",
    "# Calculate the loss using the decoder softmax \n",
    "# output and the decoder loss target\n",
    "loss = #TODO\n",
    "print(\"Loss:\", loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class encoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, batch_size):\n",
    "        super(encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax()\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, x):\n",
    "        e = self.embedding(x)\n",
    "        e = e.view(len(x), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        return out, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class decoder(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, batch_size):\n",
    "        super(decoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = Embedding(num_embeddings=vocab_size, \n",
    "                                   embedding_dim=embedding_dim)\n",
    "        self.rnn = LSTM(input_size=embedding_dim, \n",
    "                       hidden_size=hidden_dim)\n",
    "        self.linear = Linear(hidden_dim, output_dim)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = LogSoftmax(dim=1)\n",
    "        self.hidden = self.init_hidden()\n",
    "                \n",
    "    def forward(self, input, hidden):\n",
    "        self.hidden = hidden\n",
    "        e = self.embedding(input)\n",
    "        e = e.view(len(input), self.batch_size, -1)\n",
    "        out, self.hidden = self.rnn(e, self.hidden)\n",
    "        self.out = out\n",
    "        output = self.linear(out[0])\n",
    "        so = self.softmax(output)\n",
    "        return so, self.hidden\n",
    "                  \n",
    "    def init_hidden(self):\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        return (h0, c0)\n",
    "    \n",
    "class seq2seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(seq2seq, self).__init__()\n",
    "        self.enc = encoder\n",
    "        self.dec = decoder\n",
    "                \n",
    "    def forward(self, input_seq, output_seq, p_tf=0):\n",
    "        outputs = []\n",
    "        \n",
    "        self.enc.hidden = self.enc.init_hidden()\n",
    "        self.dec.hidden = self.dec.init_hidden()        \n",
    "        \n",
    "        enc_output, enc_hidden = enc.forward(torch.LongTensor(input_seq))\n",
    "        dec_hidden = enc_hidden\n",
    "        tf_cnt = 0\n",
    "        for i in range(output_seq.shape[0]):  \n",
    "            \n",
    "            if (np.random.uniform()) > p_tf and (i != 0):\n",
    "                dec_input = torch.LongTensor([torch.argmax(dec_output).data])\n",
    "            else:\n",
    "                dec_input = torch.LongTensor([output_seq[i]])\n",
    "                \n",
    "            dec_output, dec_hidden = self.dec.forward(dec_input, dec_hidden) \n",
    "            outputs.append(dec_output)\n",
    "            \n",
    "        return torch.stack(outputs).squeeze(1)\n",
    "    \n",
    "    def predict(self, input_seq, sos_idx, eos_idx, max_len=20):\n",
    "        outputs = []\n",
    "        self.enc.hidden = self.enc.init_hidden()\n",
    "        self.dec.hidden = self.dec.init_hidden()   \n",
    "        \n",
    "        enc_output, enc_hidden = enc.forward(torch.LongTensor(input_seq))\n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        cnt = 0\n",
    "        dec_input = torch.LongTensor([sos_idx])\n",
    "        \n",
    "        dec_output, dec_hidden = self.dec.forward(dec_input, dec_hidden) \n",
    "        \n",
    "        output_idx = torch.argmax(dec_output).data\n",
    "        \n",
    "        while (int(output_idx) != eos_idx) and (cnt <= max_len): \n",
    "            cnt += 1\n",
    "            dec_input = torch.LongTensor([output_idx])        \n",
    "            dec_output, dec_hidden = self.dec.forward(dec_input, dec_hidden) \n",
    "            output_idx = torch.argmax(dec_output).data\n",
    "            outputs.append(int(output_idx))\n",
    "            \n",
    "            \n",
    "        return outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "enc_vocab_size = input_size\n",
    "enc_embedding_dim = 100\n",
    "enc_hidden_dim = 50\n",
    "\n",
    "dec_vocab_size = output_size\n",
    "dec_embedding_dim = 100\n",
    "dec_hidden_dim = 50\n",
    "dec_output_dim = output_size\n",
    "\n",
    "enc = encoder(enc_vocab_size, enc_embedding_dim, enc_hidden_dim, batch_size=1)\n",
    "dec = decoder(dec_vocab_size, dec_embedding_dim, dec_hidden_dim, dec_output_dim, batch_size=1)\n",
    "s2s = seq2seq(enc, dec)\n",
    "\n",
    "\n",
    "optim = SGD(params=s2s.parameters(), lr=0.01)\n",
    "criterion = NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    s2s.train()\n",
    "    total_loss = 0\n",
    "    s2s.train()\n",
    "    for it, example in enumerate(train_data):\n",
    "\n",
    "        if (it % 100 == 0) and (it != 0):\n",
    "            print(\"Epoch|it: {}|{}, Total Loss: {:.2f}\".format(epoch, it, total_loss / it))\n",
    "        input_seq, output_seq = example\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_seq = torch.LongTensor(input_seq)\n",
    "        output_seq = torch.LongTensor(output_seq)\n",
    "\n",
    "        res = s2s.forward(input_seq, output_seq[:-1], p_tf=0.5)\n",
    "        loss = criterion(res, output_seq[1:])\n",
    "        loss.backward()\n",
    "        total_loss += loss.data.numpy()\n",
    "\n",
    "        optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sos_idx = output2idx['<SOS>']\n",
    "eos_idx = output2idx['<EOS>']\n",
    "pred_idxs = s2s.predict(input_seq, sos_idx, eos_idx)\n",
    "[idx2output[i] for i in pred_idxs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
