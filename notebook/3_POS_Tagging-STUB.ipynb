{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "import getpass\n",
    "\n",
    "from traitlets.config.manager import BaseJSONConfigManager\n",
    "path = \"/Users/{}/anaconda3/envs/rise_latest/etc/jupyter/nbconfig\".format(getpass.getuser())\n",
    "cm = BaseJSONConfigManager(config_dir=path)\n",
    "o = cm.update(\"livereveal\", {\n",
    "              \"theme\": \"sky\",\n",
    "              \"transition\": \"fade\",\n",
    "              \"start_slideshow_at\": \"selected\",\n",
    "})\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTMs for Sequence to Sequence Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixed Length Sequence to Sequence\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/Shape_of_NLP_Problems_7.png?\" alt=\"perceptron\" style=\"width:968px\">\n",
    "</center>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use Cases\n",
    "* **POS Tagging**\n",
    "* Named Entity Recognition\n",
    "* Extractive Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Before: Encoding a Sequence to a single Vector\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/0_rnn_encoding.png?\" alt=\"perceptron\" style=\"height:300px\">\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now: Encoding a Sequence to a _Sequence_ of Vectors\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/1_rnn_encoding.png?\" alt=\"perceptron\" style=\"height:300px\">\n",
    "</center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementing Part-of-Speech Tagging\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"src/1a_rnn_encoding.png?\" alt=\"perceptron\" style=\"height:300px\">\n",
    "</center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.nn import Linear, Embedding, RNN, GRU, LSTM\n",
    "from torch.nn import Sigmoid, LogSoftmax, Softmax\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss, NLLLoss, CrossEntropyLoss\n",
    "from string import punctuation\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../data/3_penn_treebank_pos.pkl')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "all_words = set(itertools.chain.from_iterable(data['text']))\n",
    "all_labels = set(itertools.chain.from_iterable(data['label']))\n",
    "\n",
    "word2idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "\n",
    "label2idx = {word: idx for idx, word in enumerate(all_labels)}\n",
    "idx2label = {idx: word for word, idx in label2idx.items()}\n",
    "\n",
    "vocab_size = len(all_words)\n",
    "label_size = len(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "features = data['text'].map(lambda x: [word2idx[i] for i in x]).tolist()\n",
    "labels = data['label'].map(lambda x: [label2idx[i] for i in x]).tolist()\n",
    "\n",
    "train_data, test_data = train_test_split(list(zip(features, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define the pieces that we'll \n",
    "# need for the model\n",
    "\n",
    "# we'll start with an embedding layer\n",
    "# the input size is the size of our vocabulary \n",
    "# (we'll need a row for every word in the input)\n",
    "# and the output size is the dimension that\n",
    "# we'll want for our word vectors\n",
    "# TODO: CREATE AN EMBEDDING LAYER\n",
    "\n",
    "# once we've converted our tokens to \n",
    "# vectors via an embedding layer, we'll\n",
    "# want to run a sequence of these vectors\n",
    "# through an LSTM layer. The input size of\n",
    "# the LSTM is our embedding dimension, \n",
    "# and the hidden dimension can be chosen by us\n",
    "# TODO: CREATE AN LSTM\n",
    "\n",
    "# because the forward pass of the LSTM\n",
    "# requires the hidden state from the previous\n",
    "# step as input, we'll have to initialize\n",
    "# the hidden state vectors. this will\n",
    "# need to be done at the beginning of each iteration\n",
    "# before we run any new sequence through the LSTM\n",
    "# TODO: CREATE A TUPLE CONTAINING THE HIDDEN\n",
    "# AND CELL STATES INITIALIZED TO ZEROS\n",
    "\n",
    "# we'll be taking the last output of \n",
    "# the LSTM sequence which will be the \n",
    "# same dimension as the hidden layer.\n",
    "# We'll then need a single linear layer \n",
    "# to act as a classifier. The input size \n",
    "# should then be the same as the hidden dim \n",
    "# of the LSTM, and the output size should be \n",
    "# the same as out number of classes for the \n",
    "# classification task\n",
    "# TODO: CREATE A LINEAR LAYER TO TRANSFORM THE LSTM OUTPUT\n",
    "\n",
    "# lastly, we'll want to normalize the final output\n",
    "# to a softmax distribution\n",
    "# CREATE A LOGSOFTMAX TRANSFORMER TO CONVERT THE \n",
    "# LINEAR OUTPUT TO A LOGSOFTMAX DISTRIBUTION\n",
    "\n",
    "# we'll want to use NLLLoss for this again\n",
    "# TODO: CREATE AN INSTANCE OF THE NLLLOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by taking a sample feature and sample target\n",
    "f = #TODO\n",
    "t = #TODO\n",
    "\n",
    "# cast them to torch tensors\n",
    "X = #TODO\n",
    "y = #TODO\n",
    "print(\"Integer Feature Sequence Shape:\", X.shape)\n",
    "print(\"Integer Target Shape:\", y.shape)\n",
    "\n",
    "# pass the sequence through the embedding layer\n",
    "embedded_sequence = #TODO\n",
    "print(\"Embedding Sequence Shape:\", embedded_sequence.shape)\n",
    "\n",
    "# the LSTM takes input tensors of shape:\n",
    "# (seq_len, batch_size, input_dimension)\n",
    "# so we'll use the .view() method\n",
    "# of the torch tensor to reshape the embedding\n",
    "# and insert an additional dimension\n",
    "embedded_sequence = #TODO\n",
    "\n",
    "# Pass the embedded sequence through the lstm layer\n",
    "lstm_output, lstm_hidden = #TODO\n",
    "print(\"LSTM Output Shape:\", lstm_output.shape)\n",
    "\n",
    "# this time, we want to retain the entire sequence\n",
    "# of outputs from the lstm\n",
    "final_output = #TODO\n",
    "print(\"Linear Layer Input Shape:\", final_output.shape)\n",
    "\n",
    "# run the final output through the linear layer\n",
    "linear_output = #TODO\n",
    "print(\"Linear Output Shape:\", linear_output.shape)\n",
    "\n",
    "# run the linear output through the softmax activation\n",
    "softmax_output = #TODO\n",
    "print(\"Softmax Output Shape:\", softmax_output.shape)\n",
    "\n",
    "# We need to squeeze out the batch dimension of the \n",
    "# softmax output before we run through out loss function\n",
    "softmax_squeezed = #TODO\n",
    "print(\"Softmax Squeezed Shape:\", softmax_squeezed.shape)\n",
    "print(\"Target Shape:\", y.shape)\n",
    "\n",
    "# calculate the loss w.r.t. the target\n",
    "loss = #TODO\n",
    "print(\"Loss Value:\", loss.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from modules.tagging import pos_tagger\n",
    "\n",
    "model = pos_tagger(vocab_size = vocab_size, \n",
    "                       embedding_dim=100, \n",
    "                       hidden_dim=50, \n",
    "                       output_dim=label_size, \n",
    "                       batch_size=1)\n",
    "\n",
    "optim = SGD(params=model.parameters(), lr=0.01)\n",
    "criterion = NLLLoss()\n",
    "\n",
    "for i in range(10):\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    y_true_train = []\n",
    "    y_pred_train = []\n",
    "    for it, example in enumerate(train_data):\n",
    "\n",
    "        f, t = example\n",
    "        X = torch.LongTensor(f)\n",
    "        y = torch.LongTensor(t)\n",
    "        \n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model.forward(X).squeeze(1)\n",
    "        optim.zero_grad()\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "        loss = criterion(output, y)\n",
    "        total_loss += loss.data.numpy()\n",
    "        y_true_train.extend(list(y.data.numpy()))\n",
    "        y_pred_train.extend(list(prediction.numpy()))\n",
    "        loss.backward()\n",
    "\n",
    "        optim.step()\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for example in test_data:\n",
    "        optim.zero_grad()\n",
    "        f, t = example\n",
    "        X = torch.LongTensor(f)\n",
    "        y = torch.LongTensor(t)\n",
    "\n",
    "        model.hidden = model.init_hidden()\n",
    "        output = model.forward(X).squeeze(1)\n",
    "        prediction = torch.argmax(output, dim=1)\n",
    "\n",
    "        y_true.extend(list(y.data.numpy()))\n",
    "        y_pred.extend(list(prediction.numpy()))\n",
    "\n",
    "    a = accuracy_score(y_true, y_pred)\n",
    "    a_train = accuracy_score(y_true_train, y_pred_train)\n",
    "    total_loss /= (it + 1)\n",
    "\n",
    "    print(\"Loss: {:.2f}, Training Accuracy: {:.2f}, Validation Accuracy: {:.2f}\".format(total_loss, a_train, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "sentence = \"we run home .\"\n",
    "words = sentence.lower().split()\n",
    "sample = [word2idx[i] for i in words]\n",
    "preds = [idx2label[i] for i in list(torch.argmax(model.forward(torch.LongTensor(sample)), dim=1).data.numpy().reshape(-1))]\n",
    "for word, pred in zip(words, preds):\n",
    "    print(word, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"I went for a run today\"\n",
    "words = sentence.lower().split()\n",
    "sample = [word2idx[i] for i in words]\n",
    "preds = [idx2label[i] for i in list(torch.argmax(model.forward(torch.LongTensor(sample)), dim=1).data.numpy().reshape(-1))]\n",
    "for word, pred in zip(words, preds):\n",
    "    print(word, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
